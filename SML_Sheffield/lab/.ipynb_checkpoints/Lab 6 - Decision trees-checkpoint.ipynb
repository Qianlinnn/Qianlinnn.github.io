{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision trees\n",
    "\n",
    "A [decision tree](https://en.wikipedia.org/wiki/Decision_tree_learning) can be thought of as a sequence of **hierarchical if-else statements** that test feature values to predict a class.\n",
    "\n",
    "\n",
    "In this notebook we will explore the use of the classification module in PySpark that implements Decision Trees classifiers. The [ml classification module](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml.classification) in PySpark basically comes with two classes that we will use in the Notebook, namely, the *pyspark.ml.classification.DecisionTreeClassifier* and the *pyspark.ml.classification.DecisionTreeClassifierModel*.\n",
    "\n",
    "Before using pyspark.ml we are going to use [scikit-learn](https://scikit-learn.org/stable/) for Decision Trees. This first example will allow us to understand some of the parameters in a decision tree. In the second example, we will see how those parameters are defined in PySpark.\n",
    "\n",
    "\n",
    "## Example with scikit-learn\n",
    "\n",
    "We will build a classifier that will be able to detect spam from the text in an email. The dataset that we will use is from the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/index.php), where UCI stands for University of California Irvine. The UCI repository is and has been a valuable resource in Machine Learning. It contains datasets for classification, regression, clustering and several other machine learning problems. These datasets are open source and they have been uploaded by contributors of many research articles. \n",
    "\n",
    "The particular dataset that we will use wil be referred to is the [Spambase Dataset](http://archive.ics.uci.edu/ml/datasets/Spambase). A detailed description is the previous link. The dataset contains 57 features related to word frequency, character frequency, and others related to capital letters. The description of the features and labels in the dataset is available [here](http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.names). The output label indicated whether an email was considered 'ham' or 'spam', so it is a binary label. \n",
    "\n",
    "We will use Decision tree as our predictive model. But first, we upload the data and the names of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "spam_data = pd.read_csv('../Data/spambase.data')\n",
    "spam_names = [spam_names.rstrip('\\n') for spam_names in open('../Data/spambase.data.names')]\n",
    "number_names = np.shape(spam_names)[0]\n",
    "for i in range(number_names):\n",
    "    local = spam_names[i]\n",
    "    colon_pos = local.find(':')\n",
    "    spam_names[i] = local[:colon_pos]\n",
    "spam_data.columns = spam_names\n",
    "X = spam_data.iloc[:, 0:57]\n",
    "y = spam_data.iloc[:, 57]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the whole dataset to build a decision tree classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=3)\n",
    "clf = clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=3,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the tree, we can use the [Graphviz](http://www.graphviz.org/) package and use the exporter [export_graphviz](https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html#sklearn.tree.export_graphviz). First, we need to install the package. An easy way to do it is to use `conda`\n",
    "\n",
    "`conda install python-graphviz`\n",
    "\n",
    "We will export the tree as a pdf file spam.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spam.pdf'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import graphviz \n",
    "dot_data = tree.export_graphviz(clf, out_file=None) \n",
    "graph = graphviz.Source(dot_data) \n",
    "graph.render(\"spam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = -()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the [export_graphviz](https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html#sklearn.tree.export_graphviz) to customize several aspects of the tree. For example, if you look at the .pdf file generated, the names of the features are assigned by default by refering to the column index in `X`. It is possible to assign the names of the features directly. Likewise for the labels `y`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                      feature_names=spam_names[0:57],  \n",
    "                      class_names=spam_names[57],  \n",
    "                      filled=True, rounded=True,  \n",
    "                      special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of printing a new .pdf file, we can render the graph inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\r\n",
       " -->\r\n",
       "<!-- Title: Tree Pages: 1 -->\r\n",
       "<svg width=\"1074pt\" height=\"433pt\"\r\n",
       " viewBox=\"0.00 0.00 1074.00 433.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 429)\">\r\n",
       "<title>Tree</title>\r\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-429 1070,-429 1070,4 -4,4\"/>\r\n",
       "<!-- 0 -->\r\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\r\n",
       "<path fill=\"#f6d3ba\" stroke=\"black\" d=\"M621,-425C621,-425 499,-425 499,-425 493,-425 487,-419 487,-413 487,-413 487,-354 487,-354 487,-348 493,-342 499,-342 499,-342 621,-342 621,-342 627,-342 633,-348 633,-354 633,-354 633,-413 633,-413 633,-419 627,-425 621,-425\"/>\r\n",
       "<text text-anchor=\"start\" x=\"495\" y=\"-409.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">char_freq_$ ≤ 0.056</text>\r\n",
       "<text text-anchor=\"start\" x=\"510\" y=\"-394.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.967</text>\r\n",
       "<text text-anchor=\"start\" x=\"508.5\" y=\"-379.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 4600</text>\r\n",
       "<text text-anchor=\"start\" x=\"495\" y=\"-364.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [2788, 1812]</text>\r\n",
       "<text text-anchor=\"start\" x=\"531.5\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = s</text>\r\n",
       "</g>\r\n",
       "<!-- 1 -->\r\n",
       "<g id=\"node2\" class=\"node\"><title>1</title>\r\n",
       "<path fill=\"#eda876\" stroke=\"black\" d=\"M539.5,-306C539.5,-306 374.5,-306 374.5,-306 368.5,-306 362.5,-300 362.5,-294 362.5,-294 362.5,-235 362.5,-235 362.5,-229 368.5,-223 374.5,-223 374.5,-223 539.5,-223 539.5,-223 545.5,-223 551.5,-229 551.5,-235 551.5,-235 551.5,-294 551.5,-294 551.5,-300 545.5,-306 539.5,-306\"/>\r\n",
       "<text text-anchor=\"start\" x=\"370.5\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">word_freq_remove ≤ 0.055</text>\r\n",
       "<text text-anchor=\"start\" x=\"407\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.786</text>\r\n",
       "<text text-anchor=\"start\" x=\"405.5\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3470</text>\r\n",
       "<text text-anchor=\"start\" x=\"396\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [2655, 815]</text>\r\n",
       "<text text-anchor=\"start\" x=\"428.5\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = s</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;1 -->\r\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;1</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M524.265,-341.907C516.275,-332.832 507.726,-323.121 499.494,-313.769\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"501.907,-311.214 492.672,-306.021 496.653,-315.839 501.907,-311.214\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"491.084\" y=\"-327.27\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\r\n",
       "</g>\r\n",
       "<!-- 8 -->\r\n",
       "<g id=\"node9\" class=\"node\"><title>8</title>\r\n",
       "<path fill=\"#53aae8\" stroke=\"black\" d=\"M722,-306C722,-306 606,-306 606,-306 600,-306 594,-300 594,-294 594,-294 594,-235 594,-235 594,-229 600,-223 606,-223 606,-223 722,-223 722,-223 728,-223 734,-229 734,-235 734,-235 734,-294 734,-294 734,-300 728,-306 722,-306\"/>\r\n",
       "<text text-anchor=\"start\" x=\"602\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">word_freq_hp ≤ 0.4</text>\r\n",
       "<text text-anchor=\"start\" x=\"614\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.523</text>\r\n",
       "<text text-anchor=\"start\" x=\"612.5\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1130</text>\r\n",
       "<text text-anchor=\"start\" x=\"607\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [133, 997]</text>\r\n",
       "<text text-anchor=\"start\" x=\"635\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = p</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;8 -->\r\n",
       "<g id=\"edge8\" class=\"edge\"><title>0&#45;&gt;8</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M596.082,-341.907C604.149,-332.832 612.781,-323.121 621.094,-313.769\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"623.954,-315.82 627.982,-306.021 618.722,-311.17 623.954,-315.82\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"629.45\" y=\"-327.278\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\r\n",
       "</g>\r\n",
       "<!-- 2 -->\r\n",
       "<g id=\"node3\" class=\"node\"><title>2</title>\r\n",
       "<path fill=\"#ea9a60\" stroke=\"black\" d=\"M280,-187C280,-187 162,-187 162,-187 156,-187 150,-181 150,-175 150,-175 150,-116 150,-116 150,-110 156,-104 162,-104 162,-104 280,-104 280,-104 286,-104 292,-110 292,-116 292,-116 292,-175 292,-175 292,-181 286,-187 280,-187\"/>\r\n",
       "<text text-anchor=\"start\" x=\"158\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">char_freq_! ≤ 0.191</text>\r\n",
       "<text text-anchor=\"start\" x=\"171\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.644</text>\r\n",
       "<text text-anchor=\"start\" x=\"169.5\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3140</text>\r\n",
       "<text text-anchor=\"start\" x=\"160\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [2625, 515]</text>\r\n",
       "<text text-anchor=\"start\" x=\"192.5\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = s</text>\r\n",
       "</g>\r\n",
       "<!-- 1&#45;&gt;2 -->\r\n",
       "<g id=\"edge2\" class=\"edge\"><title>1&#45;&gt;2</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M375.121,-222.907C351.157,-211.027 324.994,-198.056 301.139,-186.23\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"302.633,-183.064 292.119,-181.758 299.524,-189.336 302.633,-183.064\"/>\r\n",
       "</g>\r\n",
       "<!-- 5 -->\r\n",
       "<g id=\"node6\" class=\"node\"><title>5</title>\r\n",
       "<path fill=\"#4da7e8\" stroke=\"black\" d=\"M534.5,-187C534.5,-187 379.5,-187 379.5,-187 373.5,-187 367.5,-181 367.5,-175 367.5,-175 367.5,-116 367.5,-116 367.5,-110 373.5,-104 379.5,-104 379.5,-104 534.5,-104 534.5,-104 540.5,-104 546.5,-110 546.5,-116 546.5,-116 546.5,-175 546.5,-175 546.5,-181 540.5,-187 534.5,-187\"/>\r\n",
       "<text text-anchor=\"start\" x=\"375.5\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">word_freq_george ≤ 0.14</text>\r\n",
       "<text text-anchor=\"start\" x=\"407\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.439</text>\r\n",
       "<text text-anchor=\"start\" x=\"409.5\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 330</text>\r\n",
       "<text text-anchor=\"start\" x=\"404\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [30, 300]</text>\r\n",
       "<text text-anchor=\"start\" x=\"428\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = p</text>\r\n",
       "</g>\r\n",
       "<!-- 1&#45;&gt;5 -->\r\n",
       "<g id=\"edge5\" class=\"edge\"><title>1&#45;&gt;5</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M457,-222.907C457,-214.649 457,-205.864 457,-197.302\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"460.5,-197.021 457,-187.021 453.5,-197.021 460.5,-197.021\"/>\r\n",
       "</g>\r\n",
       "<!-- 3 -->\r\n",
       "<g id=\"node4\" class=\"node\"><title>3</title>\r\n",
       "<path fill=\"#e78c4b\" stroke=\"black\" d=\"M126,-68C126,-68 12,-68 12,-68 6,-68 0,-62 0,-56 0,-56 0,-12 0,-12 0,-6 6,-0 12,-0 12,-0 126,-0 126,-0 132,-0 138,-6 138,-12 138,-12 138,-56 138,-56 138,-62 132,-68 126,-68\"/>\r\n",
       "<text text-anchor=\"start\" x=\"19\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.412</text>\r\n",
       "<text text-anchor=\"start\" x=\"17.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2524</text>\r\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [2315, 209]</text>\r\n",
       "<text text-anchor=\"start\" x=\"40.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = s</text>\r\n",
       "</g>\r\n",
       "<!-- 2&#45;&gt;3 -->\r\n",
       "<g id=\"edge3\" class=\"edge\"><title>2&#45;&gt;3</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M164.401,-103.726C150.98,-94.0582 136.695,-83.767 123.376,-74.172\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"125.384,-71.305 115.224,-68.2996 121.292,-76.9846 125.384,-71.305\"/>\r\n",
       "</g>\r\n",
       "<!-- 4 -->\r\n",
       "<g id=\"node5\" class=\"node\"><title>4</title>\r\n",
       "<path fill=\"#fffdfc\" stroke=\"black\" d=\"M274,-68C274,-68 168,-68 168,-68 162,-68 156,-62 156,-56 156,-56 156,-12 156,-12 156,-6 162,-0 168,-0 168,-0 274,-0 274,-0 280,-0 286,-6 286,-12 286,-12 286,-56 286,-56 286,-62 280,-68 274,-68\"/>\r\n",
       "<text text-anchor=\"start\" x=\"179\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 1.0</text>\r\n",
       "<text text-anchor=\"start\" x=\"173.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 616</text>\r\n",
       "<text text-anchor=\"start\" x=\"164\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [310, 306]</text>\r\n",
       "<text text-anchor=\"start\" x=\"192.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = s</text>\r\n",
       "</g>\r\n",
       "<!-- 2&#45;&gt;4 -->\r\n",
       "<g id=\"edge4\" class=\"edge\"><title>2&#45;&gt;4</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M221,-103.726C221,-95.5175 221,-86.8595 221,-78.56\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"224.5,-78.2996 221,-68.2996 217.5,-78.2996 224.5,-78.2996\"/>\r\n",
       "</g>\r\n",
       "<!-- 6 -->\r\n",
       "<g id=\"node7\" class=\"node\"><title>6</title>\r\n",
       "<path fill=\"#44a3e6\" stroke=\"black\" d=\"M414,-68C414,-68 316,-68 316,-68 310,-68 304,-62 304,-56 304,-56 304,-12 304,-12 304,-6 310,-0 316,-0 316,-0 414,-0 414,-0 420,-0 426,-6 426,-12 426,-12 426,-56 426,-56 426,-62 420,-68 414,-68\"/>\r\n",
       "<text text-anchor=\"start\" x=\"315\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.302</text>\r\n",
       "<text text-anchor=\"start\" x=\"317.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 317</text>\r\n",
       "<text text-anchor=\"start\" x=\"312\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [17, 300]</text>\r\n",
       "<text text-anchor=\"start\" x=\"336\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = p</text>\r\n",
       "</g>\r\n",
       "<!-- 5&#45;&gt;6 -->\r\n",
       "<g id=\"edge6\" class=\"edge\"><title>5&#45;&gt;6</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M422.743,-103.726C415.156,-94.6966 407.113,-85.1235 399.516,-76.0816\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"402.09,-73.7045 392.978,-68.2996 396.731,-78.2075 402.09,-73.7045\"/>\r\n",
       "</g>\r\n",
       "<!-- 7 -->\r\n",
       "<g id=\"node8\" class=\"node\"><title>7</title>\r\n",
       "<path fill=\"#e58139\" stroke=\"black\" d=\"M537.5,-68C537.5,-68 456.5,-68 456.5,-68 450.5,-68 444.5,-62 444.5,-56 444.5,-56 444.5,-12 444.5,-12 444.5,-6 450.5,-0 456.5,-0 456.5,-0 537.5,-0 537.5,-0 543.5,-0 549.5,-6 549.5,-12 549.5,-12 549.5,-56 549.5,-56 549.5,-62 543.5,-68 537.5,-68\"/>\r\n",
       "<text text-anchor=\"start\" x=\"455\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.0</text>\r\n",
       "<text text-anchor=\"start\" x=\"453.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 13</text>\r\n",
       "<text text-anchor=\"start\" x=\"452.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [13, 0]</text>\r\n",
       "<text text-anchor=\"start\" x=\"468.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = s</text>\r\n",
       "</g>\r\n",
       "<!-- 5&#45;&gt;7 -->\r\n",
       "<g id=\"edge7\" class=\"edge\"><title>5&#45;&gt;7</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M471.895,-103.726C474.993,-95.2439 478.267,-86.2819 481.39,-77.7312\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"484.692,-78.8935 484.836,-68.2996 478.117,-76.4916 484.692,-78.8935\"/>\r\n",
       "</g>\r\n",
       "<!-- 9 -->\r\n",
       "<g id=\"node10\" class=\"node\"><title>9</title>\r\n",
       "<path fill=\"#47a4e7\" stroke=\"black\" d=\"M730.5,-187C730.5,-187 597.5,-187 597.5,-187 591.5,-187 585.5,-181 585.5,-175 585.5,-175 585.5,-116 585.5,-116 585.5,-110 591.5,-104 597.5,-104 597.5,-104 730.5,-104 730.5,-104 736.5,-104 742.5,-110 742.5,-116 742.5,-116 742.5,-175 742.5,-175 742.5,-181 736.5,-187 730.5,-187\"/>\r\n",
       "<text text-anchor=\"start\" x=\"593.5\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">word_freq_edu ≤ 0.49</text>\r\n",
       "<text text-anchor=\"start\" x=\"614\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.351</text>\r\n",
       "<text text-anchor=\"start\" x=\"612.5\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1060</text>\r\n",
       "<text text-anchor=\"start\" x=\"611\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [70, 990]</text>\r\n",
       "<text text-anchor=\"start\" x=\"635\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = p</text>\r\n",
       "</g>\r\n",
       "<!-- 8&#45;&gt;9 -->\r\n",
       "<g id=\"edge9\" class=\"edge\"><title>8&#45;&gt;9</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M664,-222.907C664,-214.649 664,-205.864 664,-197.302\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"667.5,-197.021 664,-187.021 660.5,-197.021 667.5,-197.021\"/>\r\n",
       "</g>\r\n",
       "<!-- 12 -->\r\n",
       "<g id=\"node13\" class=\"node\"><title>12</title>\r\n",
       "<path fill=\"#e88f4f\" stroke=\"black\" d=\"M972.5,-187C972.5,-187 807.5,-187 807.5,-187 801.5,-187 795.5,-181 795.5,-175 795.5,-175 795.5,-116 795.5,-116 795.5,-110 801.5,-104 807.5,-104 807.5,-104 972.5,-104 972.5,-104 978.5,-104 984.5,-110 984.5,-116 984.5,-116 984.5,-175 984.5,-175 984.5,-181 978.5,-187 972.5,-187\"/>\r\n",
       "<text text-anchor=\"start\" x=\"803.5\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">word_freq_remove ≤ 0.075</text>\r\n",
       "<text text-anchor=\"start\" x=\"840\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.469</text>\r\n",
       "<text text-anchor=\"start\" x=\"846.5\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 70</text>\r\n",
       "<text text-anchor=\"start\" x=\"845.5\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [63, 7]</text>\r\n",
       "<text text-anchor=\"start\" x=\"861.5\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = s</text>\r\n",
       "</g>\r\n",
       "<!-- 8&#45;&gt;12 -->\r\n",
       "<g id=\"edge12\" class=\"edge\"><title>8&#45;&gt;12</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M734.269,-227.122C755.889,-215.929 779.956,-203.47 802.58,-191.757\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"804.232,-194.843 811.504,-187.137 801.014,-188.627 804.232,-194.843\"/>\r\n",
       "</g>\r\n",
       "<!-- 10 -->\r\n",
       "<g id=\"node11\" class=\"node\"><title>10</title>\r\n",
       "<path fill=\"#44a2e6\" stroke=\"black\" d=\"M678,-68C678,-68 580,-68 580,-68 574,-68 568,-62 568,-56 568,-56 568,-12 568,-12 568,-6 574,-0 580,-0 580,-0 678,-0 678,-0 684,-0 690,-6 690,-12 690,-12 690,-56 690,-56 690,-62 684,-68 678,-68\"/>\r\n",
       "<text text-anchor=\"start\" x=\"579\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.297</text>\r\n",
       "<text text-anchor=\"start\" x=\"577.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1045</text>\r\n",
       "<text text-anchor=\"start\" x=\"576\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [55, 990]</text>\r\n",
       "<text text-anchor=\"start\" x=\"600\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = p</text>\r\n",
       "</g>\r\n",
       "<!-- 9&#45;&gt;10 -->\r\n",
       "<g id=\"edge10\" class=\"edge\"><title>9&#45;&gt;10</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M650.967,-103.726C648.285,-95.3351 645.453,-86.4745 642.747,-78.0072\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"646.022,-76.7592 639.644,-68.2996 639.355,-78.8905 646.022,-76.7592\"/>\r\n",
       "</g>\r\n",
       "<!-- 11 -->\r\n",
       "<g id=\"node12\" class=\"node\"><title>11</title>\r\n",
       "<path fill=\"#e58139\" stroke=\"black\" d=\"M801.5,-68C801.5,-68 720.5,-68 720.5,-68 714.5,-68 708.5,-62 708.5,-56 708.5,-56 708.5,-12 708.5,-12 708.5,-6 714.5,-0 720.5,-0 720.5,-0 801.5,-0 801.5,-0 807.5,-0 813.5,-6 813.5,-12 813.5,-12 813.5,-56 813.5,-56 813.5,-62 807.5,-68 801.5,-68\"/>\r\n",
       "<text text-anchor=\"start\" x=\"719\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.0</text>\r\n",
       "<text text-anchor=\"start\" x=\"717.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 15</text>\r\n",
       "<text text-anchor=\"start\" x=\"716.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [15, 0]</text>\r\n",
       "<text text-anchor=\"start\" x=\"732.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = s</text>\r\n",
       "</g>\r\n",
       "<!-- 9&#45;&gt;11 -->\r\n",
       "<g id=\"edge11\" class=\"edge\"><title>9&#45;&gt;11</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M700.119,-103.726C708.199,-94.6054 716.77,-84.93 724.851,-75.8078\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"727.491,-78.1058 731.502,-68.2996 722.251,-73.4642 727.491,-78.1058\"/>\r\n",
       "</g>\r\n",
       "<!-- 13 -->\r\n",
       "<g id=\"node14\" class=\"node\"><title>13</title>\r\n",
       "<path fill=\"#e5833c\" stroke=\"black\" d=\"M936,-68C936,-68 844,-68 844,-68 838,-68 832,-62 832,-56 832,-56 832,-12 832,-12 832,-6 838,-0 844,-0 844,-0 936,-0 936,-0 942,-0 948,-6 948,-12 948,-12 948,-56 948,-56 948,-62 942,-68 936,-68\"/>\r\n",
       "<text text-anchor=\"start\" x=\"840\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.116</text>\r\n",
       "<text text-anchor=\"start\" x=\"846.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 64</text>\r\n",
       "<text text-anchor=\"start\" x=\"845.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [63, 1]</text>\r\n",
       "<text text-anchor=\"start\" x=\"861.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = s</text>\r\n",
       "</g>\r\n",
       "<!-- 12&#45;&gt;13 -->\r\n",
       "<g id=\"edge13\" class=\"edge\"><title>12&#45;&gt;13</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M890,-103.726C890,-95.5175 890,-86.8595 890,-78.56\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"893.5,-78.2996 890,-68.2996 886.5,-78.2996 893.5,-78.2996\"/>\r\n",
       "</g>\r\n",
       "<!-- 14 -->\r\n",
       "<g id=\"node15\" class=\"node\"><title>14</title>\r\n",
       "<path fill=\"#399de5\" stroke=\"black\" d=\"M1054,-68C1054,-68 978,-68 978,-68 972,-68 966,-62 966,-56 966,-56 966,-12 966,-12 966,-6 972,-0 978,-0 978,-0 1054,-0 1054,-0 1060,-0 1066,-6 1066,-12 1066,-12 1066,-56 1066,-56 1066,-62 1060,-68 1054,-68\"/>\r\n",
       "<text text-anchor=\"start\" x=\"974\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">entropy = 0.0</text>\r\n",
       "<text text-anchor=\"start\" x=\"976.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 6</text>\r\n",
       "<text text-anchor=\"start\" x=\"975.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 6]</text>\r\n",
       "<text text-anchor=\"start\" x=\"987\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = p</text>\r\n",
       "</g>\r\n",
       "<!-- 12&#45;&gt;14 -->\r\n",
       "<g id=\"edge14\" class=\"edge\"><title>12&#45;&gt;14</title>\r\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M936.918,-103.726C947.728,-94.3318 959.215,-84.349 969.986,-74.9883\"/>\r\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"972.43,-77.501 977.683,-68.2996 967.839,-72.2174 972.43,-77.501\"/>\r\n",
       "</g>\r\n",
       "</g>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x1962d488c88>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One could quickly use the tree generated for the purposes of [feature selection](https://en.wikipedia.org/wiki/Feature_selection). Feature selection is a whole research area in machine learning with a very practical purpose: being able to identify which features are more relevant in a prediction problem. In modern big data applications, the amount of features generated is huge. For example, one could extract thousands of millions of features from a genome sequence that maps to a particular medical disorder. Finding which features are more relevant for correctly classifying the disorder could lead to breakthroughs in medicine. \n",
    "\n",
    "Relevant features can be identified starting from the top level of the tree and going down to the leaf nodes. For example, one can argue that the most important feature is the one used in the root node (e.g. char_freq_$ for the Spambase dataset) since it has the highest entropy.    \n",
    "\n",
    "## Evaluating the prediction ability of a decision tree classifier\n",
    "\n",
    "We will now evaluate the predictive ability of the decision tree classifier on a subset of the Spam dataset. The Decision Tree has several tunable parameters, including, the criterion or impurity measure (criterion) and the maximum depth of the tree (max_depth). A complete list of parameters for the DecisionTreeClassifier implemented in scikit-learn can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier).\n",
    "\n",
    "We will create a simple Grid Search for crosvalidating the best parameters for our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first split the data into a train and a test set. \n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "ss = ShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "indexes = list(ss.split(X, y)) # 根据X和y生成一行训练集和测试集的index\n",
    "train_set  = indexes[0][0]\n",
    "test_set  = indexes[0][1]\n",
    "Xtrain = X.iloc[train_set, :] # 生成训练集X\n",
    "ytrain = y.iloc[train_set] # 生成训练集y\n",
    "Xtest = X.iloc[test_set, :]\n",
    "ytest = y.iloc[test_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a Grid search for the parameters criterion and max_depth and we use the training data to find the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=ShuffleSplit(n_splits=1, random_state=42, test_size=0.3, train_size=None),\n",
       "             error_score='raise-deprecating',\n",
       "             estimator=DecisionTreeClassifier(class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features=None,\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              presort=False, random_state=None,\n",
       "                                              splitter='best'),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'criterion': array(['entropy', 'gini'], dtype='<U7'),\n",
       "                         'max_depth': [3, 5, 10, 15]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV # For model selection\n",
    "criterion_opts = np.array(['entropy', 'gini'])\n",
    "max_depth_opts = [3, 5, 10, 15]\n",
    "param_grid = dict(criterion = criterion_opts, max_depth = max_depth_opts)\n",
    "cv = ShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n",
    "grid = GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, cv=cv, scoring='accuracy')\n",
    "grid.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see now which ones were the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini', 'max_depth': 10}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then train a new decision tree using those parameters and then evaluate the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(criterion=grid.best_params_[\"criterion\"],max_depth=grid.best_params_[\"max_depth\"])\n",
    "clf.fit(Xtrain, ytrain)\n",
    "ypred = clf.predict(Xtest)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now evaluate the accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score # Accuracy score\n",
    "accuracy = accuracy_score(ytest, ypred)\n",
    "print(accuracy) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of the labels\n",
    "\n",
    "Let us look at the distribution of the instances for class in the original dataset and in the training data. \n",
    "\n",
    ">**Warning for MAC OS users** As of today, the current installation of `graphviz` **may** put its own version of freetype into the default python runtime library path. However, matplotlib needs a different version of the same library. The fix is explained in [this entry of stackoverflow](https://stackoverflow.com/questions/28028786/matplotlib-error-libfreetype-6-dylib). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "y.hist()\n",
    "plt.ylabel('Number of instances')\n",
    "plt.xlabel('Class label')\n",
    "plt.title('Class distribution for X')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ytrain.hist()\n",
    "plt.ylabel('Number of instances')\n",
    "plt.xlabel('Class label')\n",
    "plt.title('Class distribution for Xtrain')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the numbers of data observations per class are different. We usually refer to this kind of problems as \"imbalanced\". Bear in mind that when creating your training data, you need to account for this imbalance and be aware against misleading results. For example, accuracy is probably not a good metric to measure performance in these problems. Say you have a binary classification problem for which your test data consists of 100 instances. From those, 99 instances belong to class 1, say 'ham', and 1 instance belong to class 2, say 'spam'. When you test your classifier on these instances, you could have a classifier that only is able to predict class 1 correctly, but elements of class 2 are predicted incorrectly. If you apply this classifier to your test data, you would get 99% accuracy. Looking at this percentage out of context and saying that you have produced a highly realiable classifier is completely wrong since your classifier only learned to correctly classify instances in class 1. The problem gets worse if your test set is large, say 100,000 instances, 99,000 are correctly predicted for class 1 and 1,000 are incorrectly predicted for class 2. Even when the classifier fails in correctly predicted 1,000 instances from class 2, the accuracy of this classifier would be 99%.\n",
    "\n",
    "There are different [strategies for balancing a dataset](https://books.google.co.uk/books/about/Imbalanced_Learning.html?id=YqQJngEACAAJ&redir_esc=y) but if for some reason you are not able to balance it, it is important to be aware of these issues. Two stategies to use are: i) instead of simply splitting the data blindly, split the data accounting for the imbalance. sckiti-learn offers the class [StratifiedShuffleSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html) for this. Using stratified sampling is also important for regression problems. ii) do not trust \"accuracy\" as the sole metric to measure performance. Other classification metrics more suitable for imbalanced problems include [recall, precision, F1](https://en.wikipedia.org/wiki/Precision_and_recall), and [area under the curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic). \n",
    "\n",
    "### Question 1\n",
    "\n",
    "Repeat the spam prediction problem above but instead of splitting the data using ShuffleSplit, use StratifiedShuffleSplit. Do you notice any difference in the accuracy? Also, use a performance measure that takes into account imbalanced data. Do you see any difference between this new performance measure when spliting with StratifiedShuffleSplit and when sppliting using ShuffleSplit?\n",
    "\n",
    "## DecisionTreeClassifier in PySpark\n",
    "\n",
    "There are several challenges when implementing decision trees in a distributed setting, particularly when we want to use [commodity hardware](https://en.wikipedia.org/wiki/Commodity_computing). A very popular implementation is known as [PLANET: Massively Parallel Learning of Tree Ensembles with MapReduce](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36296.pdf). PLANET allows an efficient implementation of decision trees at large scale purely using map() and reduce() operations, suitable for a Hadoop cluster. Although the Decision Tree implementation in [Apache Spark borrows some of the ideas from PLANET](http://jmlr.org/papers/volume17/15-237/15-237.pdf), it also introduces additional tricks that exploit the well-known advantages of Apache Spark, e.g. in memory computing. The Apache Spark implementation of the DecisionTree classifier may not be as flexible as the scikit-learn one (bear in mind they were designed under a different sets of restrictions), but it still allows the use of such a powerful machine learning model at large scale.\n",
    "\n",
    "You can find more technical details on the implementation of Decision Trees in Apache Spark in the youtube video [Scalable Decision Trees in Spark MLlib](https://www.youtube.com/watch?v=N453EV5gHRA&t=10m30s) by Manish Amde and the youtube video [Decision Trees on Spark](https://www.youtube.com/watch?v=3WS9OK3EXVA) by Joseph Bradley. These technical details are also reviewed in a [blog post on decision trees](https://databricks.com/blog/2014/09/29/scalable-decision-trees-in-mllib.html) and another [blog post on random forests](https://databricks.com/blog/2015/01/21/random-forests-and-boosting-in-mllib.html). \n",
    "\n",
    "We start by creating a <tt>SparkSession</tt> (unless you are running in a pyspark shell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import findspark\n",
    "# findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "spark = SparkSession.builder.master(\"local[2]\").appName(\"COM6012 Decision Trees\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now load the dataset and load the names of the features and label that we will use to create the schema for the dataframe. We also cache the dataframe since we are going to perform several operations to rawdata inside a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata = spark.read.csv('../Data/spambase.data')\n",
    "rawdata.cache()\n",
    "ncolumns = len(rawdata.columns)\n",
    "spam_names = [spam_names.rstrip('\\n') for spam_names in open('../Data/spambase.data.names')]\n",
    "number_names = np.shape(spam_names)[0]\n",
    "for i in range(number_names):\n",
    "    local = spam_names[i]\n",
    "    colon_pos = local.find(':')\n",
    "    spam_names[i] = local[:colon_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['word_freq_make', 'word_freq_address', 'word_freq_all',\n",
       "       'word_freq_3d', 'word_freq_our', 'word_freq_over',\n",
       "       'word_freq_remove', 'word_freq_internet', 'word_freq_order',\n",
       "       'word_freq_mail', 'word_freq_receive', 'word_freq_will',\n",
       "       'word_freq_people', 'word_freq_report', 'word_freq_addresses',\n",
       "       'word_freq_free', 'word_freq_business', 'word_freq_email',\n",
       "       'word_freq_you', 'word_freq_credit', 'word_freq_your',\n",
       "       'word_freq_font', 'word_freq_000', 'word_freq_money',\n",
       "       'word_freq_hp', 'word_freq_hpl', 'word_freq_george',\n",
       "       'word_freq_650', 'word_freq_lab', 'word_freq_labs',\n",
       "       'word_freq_telnet', 'word_freq_857', 'word_freq_data',\n",
       "       'word_freq_415', 'word_freq_85', 'word_freq_technology',\n",
       "       'word_freq_1999', 'word_freq_parts', 'word_freq_pm',\n",
       "       'word_freq_direct', 'word_freq_cs', 'word_freq_meeting',\n",
       "       'word_freq_original', 'word_freq_project', 'word_freq_re',\n",
       "       'word_freq_edu', 'word_freq_table', 'word_freq_conference',\n",
       "       'char_freq_;', 'char_freq_(', 'char_freq_[', 'char_freq_!',\n",
       "       'char_freq_$', 'char_freq_#', 'capital_run_length_average',\n",
       "       'capital_run_length_longest', 'capital_run_length_total',\n",
       "       'spam_ham'], dtype='<U26')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(spam_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the [<tt>withColumnRenamed</tt>](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=withcolumn#pyspark.sql.DataFrame.withColumnRenamed) method for the dataframe to rename the columns using the more familiar names for the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemaNames = rawdata.schema.names  #重命名最后一列的名字\n",
    "spam_names[ncolumns-1] = 'labels'\n",
    "for i in range(ncolumns):\n",
    "    rawdata = rawdata.withColumnRenamed(schemaNames[i], spam_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------------+-------------+------------+-------------+--------------+----------------+------------------+---------------+--------------+-----------------+--------------+----------------+----------------+-------------------+--------------+------------------+---------------+-------------+----------------+--------------+--------------+-------------+---------------+------------+-------------+----------------+-------------+-------------+--------------+----------------+-------------+--------------+-------------+------------+--------------------+--------------+---------------+------------+----------------+------------+-----------------+------------------+-----------------+------------+-------------+---------------+--------------------+-----------+-----------+-----------+-----------+-----------+-----------+--------------------------+--------------------------+------------------------+------+\n",
      "|word_freq_make|word_freq_address|word_freq_all|word_freq_3d|word_freq_our|word_freq_over|word_freq_remove|word_freq_internet|word_freq_order|word_freq_mail|word_freq_receive|word_freq_will|word_freq_people|word_freq_report|word_freq_addresses|word_freq_free|word_freq_business|word_freq_email|word_freq_you|word_freq_credit|word_freq_your|word_freq_font|word_freq_000|word_freq_money|word_freq_hp|word_freq_hpl|word_freq_george|word_freq_650|word_freq_lab|word_freq_labs|word_freq_telnet|word_freq_857|word_freq_data|word_freq_415|word_freq_85|word_freq_technology|word_freq_1999|word_freq_parts|word_freq_pm|word_freq_direct|word_freq_cs|word_freq_meeting|word_freq_original|word_freq_project|word_freq_re|word_freq_edu|word_freq_table|word_freq_conference|char_freq_;|char_freq_(|char_freq_[|char_freq_!|char_freq_$|char_freq_#|capital_run_length_average|capital_run_length_longest|capital_run_length_total|labels|\n",
      "+--------------+-----------------+-------------+------------+-------------+--------------+----------------+------------------+---------------+--------------+-----------------+--------------+----------------+----------------+-------------------+--------------+------------------+---------------+-------------+----------------+--------------+--------------+-------------+---------------+------------+-------------+----------------+-------------+-------------+--------------+----------------+-------------+--------------+-------------+------------+--------------------+--------------+---------------+------------+----------------+------------+-----------------+------------------+-----------------+------------+-------------+---------------+--------------------+-----------+-----------+-----------+-----------+-----------+-----------+--------------------------+--------------------------+------------------------+------+\n",
      "|             0|             0.64|         0.64|           0|         0.32|             0|               0|                 0|              0|             0|                0|          0.64|               0|               0|                  0|          0.32|                 0|           1.29|         1.93|               0|          0.96|             0|            0|              0|           0|            0|               0|            0|            0|             0|               0|            0|             0|            0|           0|                   0|             0|              0|           0|               0|           0|                0|                 0|                0|           0|            0|              0|                   0|          0|          0|          0|      0.778|          0|          0|                     3.756|                        61|                     278|     1|\n",
      "|          0.21|             0.28|          0.5|           0|         0.14|          0.28|            0.21|              0.07|              0|          0.94|             0.21|          0.79|            0.65|            0.21|               0.14|          0.14|              0.07|           0.28|         3.47|               0|          1.59|             0|         0.43|           0.43|           0|            0|               0|            0|            0|             0|               0|            0|             0|            0|           0|                   0|          0.07|              0|           0|               0|           0|                0|                 0|                0|           0|            0|              0|                   0|          0|      0.132|          0|      0.372|       0.18|      0.048|                     5.114|                       101|                    1028|     1|\n",
      "|          0.06|                0|         0.71|           0|         1.23|          0.19|            0.19|              0.12|           0.64|          0.25|             0.38|          0.45|            0.12|               0|               1.75|          0.06|              0.06|           1.03|         1.36|            0.32|          0.51|             0|         1.16|           0.06|           0|            0|               0|            0|            0|             0|               0|            0|             0|            0|           0|                   0|             0|              0|           0|            0.06|           0|                0|              0.12|                0|        0.06|         0.06|              0|                   0|       0.01|      0.143|          0|      0.276|      0.184|       0.01|                     9.821|                       485|                    2259|     1|\n",
      "|             0|                0|            0|           0|         0.63|             0|            0.31|              0.63|           0.31|          0.63|             0.31|          0.31|            0.31|               0|                  0|          0.31|                 0|              0|         3.18|               0|          0.31|             0|            0|              0|           0|            0|               0|            0|            0|             0|               0|            0|             0|            0|           0|                   0|             0|              0|           0|               0|           0|                0|                 0|                0|           0|            0|              0|                   0|          0|      0.137|          0|      0.137|          0|          0|                     3.537|                        40|                     191|     1|\n",
      "|             0|                0|            0|           0|         0.63|             0|            0.31|              0.63|           0.31|          0.63|             0.31|          0.31|            0.31|               0|                  0|          0.31|                 0|              0|         3.18|               0|          0.31|             0|            0|              0|           0|            0|               0|            0|            0|             0|               0|            0|             0|            0|           0|                   0|             0|              0|           0|               0|           0|                0|                 0|                0|           0|            0|              0|                   0|          0|      0.135|          0|      0.135|          0|          0|                     3.537|                        40|                     191|     1|\n",
      "|             0|                0|            0|           0|         1.85|             0|               0|              1.85|              0|             0|                0|             0|               0|               0|                  0|             0|                 0|              0|            0|               0|             0|             0|            0|              0|           0|            0|               0|            0|            0|             0|               0|            0|             0|            0|           0|                   0|             0|              0|           0|               0|           0|                0|                 0|                0|           0|            0|              0|                   0|          0|      0.223|          0|          0|          0|          0|                         3|                        15|                      54|     1|\n",
      "|             0|                0|            0|           0|         1.92|             0|               0|                 0|              0|          0.64|             0.96|          1.28|               0|               0|                  0|          0.96|                 0|           0.32|         3.85|               0|          0.64|             0|            0|              0|           0|            0|               0|            0|            0|             0|               0|            0|             0|            0|           0|                   0|             0|              0|           0|               0|           0|                0|                 0|                0|           0|            0|              0|                   0|          0|      0.054|          0|      0.164|      0.054|          0|                     1.671|                         4|                     112|     1|\n",
      "|             0|                0|            0|           0|         1.88|             0|               0|              1.88|              0|             0|                0|             0|               0|               0|                  0|             0|                 0|              0|            0|               0|             0|             0|            0|              0|           0|            0|               0|            0|            0|             0|               0|            0|             0|            0|           0|                   0|             0|              0|           0|               0|           0|                0|                 0|                0|           0|            0|              0|                   0|          0|      0.206|          0|          0|          0|          0|                      2.45|                        11|                      49|     1|\n",
      "|          0.15|                0|         0.46|           0|         0.61|             0|             0.3|                 0|           0.92|          0.76|             0.76|          0.92|               0|               0|                  0|             0|                 0|           0.15|         1.23|            3.53|             2|             0|            0|           0.15|           0|            0|               0|            0|            0|             0|               0|            0|          0.15|            0|           0|                   0|             0|              0|           0|               0|           0|                0|               0.3|                0|           0|            0|              0|                   0|          0|      0.271|          0|      0.181|      0.203|      0.022|                     9.744|                       445|                    1257|     1|\n",
      "|          0.06|             0.12|         0.77|           0|         0.19|          0.32|            0.38|                 0|           0.06|             0|                0|          0.64|            0.25|               0|               0.12|             0|                 0|           0.12|         1.67|            0.06|          0.71|             0|         0.19|              0|           0|            0|               0|            0|            0|             0|               0|            0|             0|            0|           0|                   0|             0|              0|           0|               0|           0|                0|                 0|             0.06|           0|            0|              0|                   0|       0.04|       0.03|          0|      0.244|      0.081|          0|                     1.729|                        43|                     749|     1|\n",
      "|             0|                0|            0|           0|            0|             0|            0.96|                 0|              0|          1.92|             0.96|             0|               0|               0|                  0|             0|                 0|           0.96|         3.84|               0|          0.96|             0|            0|              0|           0|            0|               0|            0|            0|             0|               0|            0|             0|            0|           0|                   0|             0|              0|           0|            0.96|           0|                0|                 0|                0|           0|            0|              0|                   0|          0|          0|          0|      0.462|          0|          0|                     1.312|                         6|                      21|     1|\n",
      "|             0|                0|         0.25|           0|         0.38|          0.25|            0.25|                 0|              0|             0|             0.12|          0.12|            0.12|               0|                  0|             0|                 0|              0|         1.16|               0|          0.77|             0|            0|              0|           0|            0|               0|            0|            0|             0|               0|            0|             0|            0|           0|                   0|             0|              0|           0|               0|           0|                0|                 0|                0|           0|            0|              0|                   0|      0.022|      0.044|          0|      0.663|          0|          0|                     1.243|                        11|                     184|     1|\n",
      "|             0|             0.69|         0.34|           0|         0.34|             0|               0|                 0|              0|             0|                0|          0.69|               0|               0|                  0|          0.34|                 0|           1.39|         2.09|               0|          1.04|             0|            0|              0|           0|            0|               0|            0|            0|             0|               0|            0|             0|            0|           0|                   0|             0|              0|           0|               0|           0|                0|                 0|                0|           0|            0|              0|                   0|          0|      0.056|          0|      0.786|          0|          0|                     3.728|                        61|                     261|     1|\n",
      "|             0|                0|            0|           0|          0.9|             0|             0.9|                 0|              0|           0.9|              0.9|             0|             0.9|               0|                  0|             0|                 0|              0|         2.72|               0|           0.9|             0|            0|              0|           0|            0|               0|            0|            0|             0|               0|            0|             0|            0|           0|                   0|             0|              0|           0|               0|           0|                0|                 0|                0|           0|            0|              0|                   0|          0|          0|          0|          0|          0|          0|                     2.083|                         7|                      25|     1|\n",
      "|             0|                0|         1.42|           0|         0.71|          0.35|               0|              0.35|              0|          0.71|                0|          0.35|               0|               0|                  0|          5.35|                 0|              0|         3.21|               0|          2.85|             0|         0.35|              0|           0|            0|               0|            0|            0|             0|               0|            0|             0|            0|           0|                   0|             0|              0|           0|               0|           0|                0|                 0|                0|           0|            0|              0|                   0|          0|      0.102|          0|      0.357|          0|          0|                     1.971|                        24|                     205|     1|\n",
      "|             0|             0.42|         0.42|           0|         1.27|             0|            0.42|                 0|              0|          1.27|                0|             0|               0|               0|                  0|          1.27|                 0|              0|          1.7|            0.42|          1.27|             0|            0|           0.42|           0|            0|               0|            0|            0|             0|               0|            0|             0|            0|           0|                   0|          1.27|              0|           0|            0.42|           0|                0|                 0|                0|           0|            0|              0|                   0|          0|      0.063|          0|      0.572|      0.063|          0|                     5.659|                        55|                     249|     1|\n",
      "|             0|                0|            0|           0|         0.94|             0|               0|                 0|              0|             0|                0|             0|               0|               0|                  0|             0|                 0|              0|         1.88|               0|          2.83|             0|            0|              0|           0|            0|               0|            0|            0|             0|               0|            0|             0|            0|           0|                   0|             0|              0|           0|               0|           0|                0|                 0|                0|           0|            0|              0|                   0|          0|          0|          0|      0.428|          0|          0|                     4.652|                        31|                     107|     1|\n",
      "|             0|                0|            0|           0|            0|             0|               0|                 0|              0|             0|                0|             0|               0|               0|                  0|             0|                 0|              0|            0|               0|          2.11|             0|          0.7|              0|           0|            0|               0|            0|            0|             0|               0|            0|             0|            0|           0|                   0|             0|              0|           0|               0|           0|                0|                 0|                0|           0|            0|              0|                   0|          0|          0|          0|      1.975|       0.37|          0|                    35.461|                        95|                     461|     1|\n",
      "|             0|                0|         0.55|           0|         1.11|             0|            0.18|                 0|              0|             0|                0|             0|            0.92|               0|               0.18|             0|              0.37|           0.37|         3.15|               0|          0.92|             0|            0|              0|           0|            0|               0|            0|            0|             0|               0|            0|             0|            0|           0|                   0|             0|              0|           0|               0|           0|                0|                 0|                0|           0|            0|              0|                   0|          0|      0.182|          0|      0.455|          0|          0|                      1.32|                         4|                      70|     1|\n",
      "|             0|             0.63|            0|           0|         1.59|          0.31|               0|                 0|           0.31|             0|                0|          0.63|               0|               0|               1.27|          0.63|              0.31|           3.18|         2.22|               0|          1.91|             0|         0.31|           0.63|           0|            0|               0|         0.31|            0|             0|               0|            0|             0|            0|           0|                   0|             0|              0|           0|            1.59|           0|                0|                 0|                0|           0|            0|              0|                   0|          0|      0.275|          0|      0.055|      0.496|          0|                     3.509|                        91|                     186|     1|\n",
      "+--------------+-----------------+-------------+------------+-------------+--------------+----------------+------------------+---------------+--------------+-----------------+--------------+----------------+----------------+-------------------+--------------+------------------+---------------+-------------+----------------+--------------+--------------+-------------+---------------+------------+-------------+----------------+-------------+-------------+--------------+----------------+-------------+--------------+-------------+------------+--------------------+--------------+---------------+------------+----------------+------------+-----------------+------------------+-----------------+------------+-------------+---------------+--------------------+-----------+-----------+-----------+-----------+-----------+-----------+--------------------------+--------------------------+------------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rawdata.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps one of the most important operations when doing data analytics in Apache Spark consists in preprocessing the dataset so that it can be analysed using the MLlib package. In the case of supervised learning, classification or regression, we want the data into a column of type `Double` for the label and a column of type `SparseVector` or `DenseVector` for the features. \n",
    "\n",
    "We import the <tt>Double</tt> type from pyspark.sql.types, use the [<tt>withColumn</tt>](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=withcolumn#pyspark.sql.DataFrame.withColumn) method for the dataframe and `cast()` the column to DoubleType."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "for i in range(ncolumns):\n",
    "    rawdata = rawdata.withColumn(spam_names[i], rawdata[spam_names[i]].cast(DoubleType())) # 将数据都变为double类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- word_freq_make: double (nullable = true)\n",
      " |-- word_freq_address: double (nullable = true)\n",
      " |-- word_freq_all: double (nullable = true)\n",
      " |-- word_freq_3d: double (nullable = true)\n",
      " |-- word_freq_our: double (nullable = true)\n",
      " |-- word_freq_over: double (nullable = true)\n",
      " |-- word_freq_remove: double (nullable = true)\n",
      " |-- word_freq_internet: double (nullable = true)\n",
      " |-- word_freq_order: double (nullable = true)\n",
      " |-- word_freq_mail: double (nullable = true)\n",
      " |-- word_freq_receive: double (nullable = true)\n",
      " |-- word_freq_will: double (nullable = true)\n",
      " |-- word_freq_people: double (nullable = true)\n",
      " |-- word_freq_report: double (nullable = true)\n",
      " |-- word_freq_addresses: double (nullable = true)\n",
      " |-- word_freq_free: double (nullable = true)\n",
      " |-- word_freq_business: double (nullable = true)\n",
      " |-- word_freq_email: double (nullable = true)\n",
      " |-- word_freq_you: double (nullable = true)\n",
      " |-- word_freq_credit: double (nullable = true)\n",
      " |-- word_freq_your: double (nullable = true)\n",
      " |-- word_freq_font: double (nullable = true)\n",
      " |-- word_freq_000: double (nullable = true)\n",
      " |-- word_freq_money: double (nullable = true)\n",
      " |-- word_freq_hp: double (nullable = true)\n",
      " |-- word_freq_hpl: double (nullable = true)\n",
      " |-- word_freq_george: double (nullable = true)\n",
      " |-- word_freq_650: double (nullable = true)\n",
      " |-- word_freq_lab: double (nullable = true)\n",
      " |-- word_freq_labs: double (nullable = true)\n",
      " |-- word_freq_telnet: double (nullable = true)\n",
      " |-- word_freq_857: double (nullable = true)\n",
      " |-- word_freq_data: double (nullable = true)\n",
      " |-- word_freq_415: double (nullable = true)\n",
      " |-- word_freq_85: double (nullable = true)\n",
      " |-- word_freq_technology: double (nullable = true)\n",
      " |-- word_freq_1999: double (nullable = true)\n",
      " |-- word_freq_parts: double (nullable = true)\n",
      " |-- word_freq_pm: double (nullable = true)\n",
      " |-- word_freq_direct: double (nullable = true)\n",
      " |-- word_freq_cs: double (nullable = true)\n",
      " |-- word_freq_meeting: double (nullable = true)\n",
      " |-- word_freq_original: double (nullable = true)\n",
      " |-- word_freq_project: double (nullable = true)\n",
      " |-- word_freq_re: double (nullable = true)\n",
      " |-- word_freq_edu: double (nullable = true)\n",
      " |-- word_freq_table: double (nullable = true)\n",
      " |-- word_freq_conference: double (nullable = true)\n",
      " |-- char_freq_;: double (nullable = true)\n",
      " |-- char_freq_(: double (nullable = true)\n",
      " |-- char_freq_[: double (nullable = true)\n",
      " |-- char_freq_!: double (nullable = true)\n",
      " |-- char_freq_$: double (nullable = true)\n",
      " |-- char_freq_#: double (nullable = true)\n",
      " |-- capital_run_length_average: double (nullable = true)\n",
      " |-- capital_run_length_longest: double (nullable = true)\n",
      " |-- capital_run_length_total: double (nullable = true)\n",
      " |-- labels: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rawdata.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now a dataframe that contains several columns corresponding to the features and the last column corresponding to the labels. We use the [<tt>VectorAssembler</tt>](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html?highlight=vectorassembler#pyspark.ml.feature.VectorAssembler) tool to concatenate all the features in a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols = spam_names[0:ncolumns-1], outputCol = 'features') \n",
    "raw_plus_vector = assembler.transform(rawdata) # 添加了1列前面的数据集并作为list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[word_freq_make: double, word_freq_address: double, word_freq_all: double, word_freq_3d: double, word_freq_our: double, word_freq_over: double, word_freq_remove: double, word_freq_internet: double, word_freq_order: double, word_freq_mail: double, word_freq_receive: double, word_freq_will: double, word_freq_people: double, word_freq_report: double, word_freq_addresses: double, word_freq_free: double, word_freq_business: double, word_freq_email: double, word_freq_you: double, word_freq_credit: double, word_freq_your: double, word_freq_font: double, word_freq_000: double, word_freq_money: double, word_freq_hp: double, word_freq_hpl: double, word_freq_george: double, word_freq_650: double, word_freq_lab: double, word_freq_labs: double, word_freq_telnet: double, word_freq_857: double, word_freq_data: double, word_freq_415: double, word_freq_85: double, word_freq_technology: double, word_freq_1999: double, word_freq_parts: double, word_freq_pm: double, word_freq_direct: double, word_freq_cs: double, word_freq_meeting: double, word_freq_original: double, word_freq_project: double, word_freq_re: double, word_freq_edu: double, word_freq_table: double, word_freq_conference: double, char_freq_;: double, char_freq_(: double, char_freq_[: double, char_freq_!: double, char_freq_$: double, char_freq_#: double, capital_run_length_average: double, capital_run_length_longest: double, capital_run_length_total: double, labels: double, features: vector]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(rawdata)\n",
    "raw_plus_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------------+\n",
      "|  a|  b|  c|     features|\n",
      "+---+---+---+-------------+\n",
      "|  1|  0|  3|[1.0,0.0,3.0]|\n",
      "+---+---+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, 0, 3)], [\"a\", \"b\", \"c\"])\n",
    "vecAssembler = VectorAssembler(inputCols=[\"a\", \"b\", \"c\"], outputCol=\"features\")\n",
    "df_data = vecAssembler.transform(df)\n",
    "df_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<tt>raw_plus_vector</tt> is now a dataframe with an additional column for the feature vectors. We create our final dataframe, that we call data, by selecting the appropriate columns from <tt>raw_plus_vector</tt> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = raw_plus_vector.select('features','labels') # 选择列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- labels: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pyspark code above was all for preprocessing the data. We can now start the machine learning analysis by creating the training and test set and then designing the DecisionTreeClassifier using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingData, testData) = data.randomSplit([0.7, 0.3], 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [DecisionTreeClassifier](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html?highlight=decisiontreeclassifier#pyspark.ml.classification.DecisionTreeClassifier) implemented in PySpark has several parameters to tune. Some of them are\n",
    "\n",
    "> **maxDepth**: it corresponds to the maximum depth of the tree. The default is 5.<p>\n",
    "**maxBins**: it determines how many bins should be created from continuous features. The default is 32.<p>\n",
    "    **impurity**: it is the metric used to compute information gain. The options are \"gini\" or \"entropy\". The default is \"gini\".<p>\n",
    "        **minInfoGain**: it determines the minimum information gain that will be used for a split. The default is zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(labelCol=\"labels\", featuresCol=\"features\", maxDepth=10, impurity='entropy')\n",
    "model = dt.fit(trainingData)\n",
    "predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally use the [MulticlassClassificationEvaluator](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html?highlight=multiclassclassificationevaluator#pyspark.ml.evaluation.MulticlassClassificationEvaluator) tool to assess the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.92371 \n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator # 进行评估\n",
    "evaluator = MulticlassClassificationEvaluator\\\n",
    "      (labelCol=\"labels\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = %g \" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The individual importance of the features can be obtained using [featureImportances](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html?highlight=featureimportances#pyspark.ml.classification.DecisionTreeClassificationModel.featureImportances). We can recover the values of importance and the indexes of the vector where these values are different from zero using values and indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = model.featureImportances # 获得重要性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_feat = np.zeros(ncolumns-1)\n",
    "imp_feat[fi.indices] = fi.values #生成imp_feat的重要性 让对应的index放置对应的fi值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the relative importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 57 artists>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(ncolumns-1)\n",
    "plt.bar(x, imp_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature with the highest importance is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'char_freq_$'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_names[np.argmax(imp_feat)] # 最重要的特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualise the DecisionTree in the form of *if-then-else* statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel (uid=DecisionTreeClassifier_832de434eef1) of depth 10 with 185 nodes\n",
      "  If (feature 52 <= 0.0525)\n",
      "   If (feature 6 <= 0.055)\n",
      "    If (feature 51 <= 0.3805)\n",
      "     If (feature 26 <= 0.005)\n",
      "      If (feature 15 <= 0.025)\n",
      "       If (feature 23 <= 0.01)\n",
      "        If (feature 24 <= 0.01)\n",
      "         If (feature 56 <= 28.5)\n",
      "          If (feature 7 <= 0.005)\n",
      "           Predict: 0.0\n",
      "          Else (feature 7 > 0.005)\n",
      "           If (feature 20 <= 0.01)\n",
      "            Predict: 0.0\n",
      "           Else (feature 20 > 0.01)\n",
      "            Predict: 1.0\n",
      "         Else (feature 56 > 28.5)\n",
      "          If (feature 4 <= 1.835)\n",
      "           Predict: 0.0\n",
      "          Else (feature 4 > 1.835)\n",
      "           If (feature 35 <= 0.005)\n",
      "            Predict: 1.0\n",
      "           Else (feature 35 > 0.005)\n",
      "            Predict: 0.0\n",
      "        Else (feature 24 > 0.01)\n",
      "         If (feature 10 <= 0.295)\n",
      "          Predict: 0.0\n",
      "         Else (feature 10 > 0.295)\n",
      "          If (feature 20 <= 0.515)\n",
      "           Predict: 0.0\n",
      "          Else (feature 20 > 0.515)\n",
      "           If (feature 1 <= 0.005)\n",
      "            Predict: 0.0\n",
      "           Else (feature 1 > 0.005)\n",
      "            Predict: 1.0\n",
      "       Else (feature 23 > 0.01)\n",
      "        If (feature 44 <= 0.385)\n",
      "         If (feature 24 <= 0.01)\n",
      "          If (feature 18 <= 5.535)\n",
      "           Predict: 1.0\n",
      "          Else (feature 18 > 5.535)\n",
      "           If (feature 20 <= 0.01)\n",
      "            Predict: 1.0\n",
      "           Else (feature 20 > 0.01)\n",
      "            Predict: 0.0\n",
      "         Else (feature 24 > 0.01)\n",
      "          Predict: 0.0\n",
      "        Else (feature 44 > 0.385)\n",
      "         Predict: 0.0\n",
      "      Else (feature 15 > 0.025)\n",
      "       If (feature 24 <= 0.025)\n",
      "        If (feature 4 <= 0.01)\n",
      "         If (feature 45 <= 0.015)\n",
      "          If (feature 55 <= 43.5)\n",
      "           If (feature 55 <= 6.5)\n",
      "            Predict: 0.0\n",
      "           Else (feature 55 > 6.5)\n",
      "            Predict: 1.0\n",
      "          Else (feature 55 > 43.5)\n",
      "           If (feature 9 <= 0.015)\n",
      "            Predict: 0.0\n",
      "           Else (feature 9 > 0.015)\n",
      "            Predict: 1.0\n",
      "         Else (feature 45 > 0.015)\n",
      "          If (feature 27 <= 0.01)\n",
      "           Predict: 0.0\n",
      "          Else (feature 27 > 0.01)\n",
      "           Predict: 1.0\n",
      "        Else (feature 4 > 0.01)\n",
      "         If (feature 0 <= 0.175)\n",
      "          If (feature 12 <= 0.545)\n",
      "           If (feature 40 <= 0.01)\n",
      "            Predict: 1.0\n",
      "           Else (feature 40 > 0.01)\n",
      "            Predict: 0.0\n",
      "          Else (feature 12 > 0.545)\n",
      "           Predict: 0.0\n",
      "         Else (feature 0 > 0.175)\n",
      "          Predict: 0.0\n",
      "       Else (feature 24 > 0.025)\n",
      "        If (feature 11 <= 0.10500000000000001)\n",
      "         If (feature 18 <= 0.605)\n",
      "          Predict: 1.0\n",
      "         Else (feature 18 > 0.605)\n",
      "          If (feature 18 <= 1.9449999999999998)\n",
      "           Predict: 0.0\n",
      "          Else (feature 18 > 1.9449999999999998)\n",
      "           Predict: 1.0\n",
      "        Else (feature 11 > 0.10500000000000001)\n",
      "         Predict: 0.0\n",
      "     Else (feature 26 > 0.005)\n",
      "      Predict: 0.0\n",
      "    Else (feature 51 > 0.3805)\n",
      "     If (feature 56 <= 52.5)\n",
      "      If (feature 15 <= 0.615)\n",
      "       If (feature 51 <= 0.9974999999999999)\n",
      "        If (feature 54 <= 3.701)\n",
      "         If (feature 23 <= 0.01)\n",
      "          Predict: 0.0\n",
      "         Else (feature 23 > 0.01)\n",
      "          Predict: 1.0\n",
      "        Else (feature 54 > 3.701)\n",
      "         Predict: 1.0\n",
      "       Else (feature 51 > 0.9974999999999999)\n",
      "        If (feature 45 <= 0.015)\n",
      "         If (feature 56 <= 17.5)\n",
      "          If (feature 18 <= 1.645)\n",
      "           Predict: 0.0\n",
      "          Else (feature 18 > 1.645)\n",
      "           If (feature 55 <= 3.5)\n",
      "            Predict: 0.0\n",
      "           Else (feature 55 > 3.5)\n",
      "            Predict: 1.0\n",
      "         Else (feature 56 > 17.5)\n",
      "          If (feature 11 <= 1.3250000000000002)\n",
      "           If (feature 24 <= 0.01)\n",
      "            Predict: 1.0\n",
      "           Else (feature 24 > 0.01)\n",
      "            Predict: 0.0\n",
      "          Else (feature 11 > 1.3250000000000002)\n",
      "           Predict: 0.0\n",
      "        Else (feature 45 > 0.015)\n",
      "         Predict: 0.0\n",
      "      Else (feature 15 > 0.615)\n",
      "       If (feature 8 <= 0.005)\n",
      "        Predict: 1.0\n",
      "       Else (feature 8 > 0.005)\n",
      "        Predict: 0.0\n",
      "     Else (feature 56 > 52.5)\n",
      "      If (feature 45 <= 0.015)\n",
      "       If (feature 51 <= 0.7745)\n",
      "        If (feature 24 <= 0.01)\n",
      "         If (feature 44 <= 0.675)\n",
      "          If (feature 18 <= 4.525)\n",
      "           Predict: 1.0\n",
      "          Else (feature 18 > 4.525)\n",
      "           Predict: 0.0\n",
      "         Else (feature 44 > 0.675)\n",
      "          Predict: 0.0\n",
      "        Else (feature 24 > 0.01)\n",
      "         Predict: 0.0\n",
      "       Else (feature 51 > 0.7745)\n",
      "        If (feature 55 <= 4.5)\n",
      "         Predict: 0.0\n",
      "        Else (feature 55 > 4.5)\n",
      "         If (feature 9 <= 0.675)\n",
      "          Predict: 1.0\n",
      "         Else (feature 9 > 0.675)\n",
      "          If (feature 0 <= 0.005)\n",
      "           Predict: 1.0\n",
      "          Else (feature 0 > 0.005)\n",
      "           Predict: 0.0\n",
      "      Else (feature 45 > 0.015)\n",
      "       Predict: 0.0\n",
      "   Else (feature 6 > 0.055)\n",
      "    If (feature 26 <= 0.005)\n",
      "     If (feature 45 <= 0.015)\n",
      "      If (feature 29 <= 0.185)\n",
      "       If (feature 15 <= 0.005)\n",
      "        If (feature 20 <= 0.375)\n",
      "         If (feature 55 <= 12.5)\n",
      "          If (feature 18 <= 0.845)\n",
      "           Predict: 1.0\n",
      "          Else (feature 18 > 0.845)\n",
      "           If (feature 10 <= 0.14500000000000002)\n",
      "            Predict: 0.0\n",
      "           Else (feature 10 > 0.14500000000000002)\n",
      "            Predict: 1.0\n",
      "         Else (feature 55 > 12.5)\n",
      "          If (feature 44 <= 0.535)\n",
      "           Predict: 1.0\n",
      "          Else (feature 44 > 0.535)\n",
      "           Predict: 0.0\n",
      "        Else (feature 20 > 0.375)\n",
      "         If (feature 24 <= 0.265)\n",
      "          Predict: 1.0\n",
      "         Else (feature 24 > 0.265)\n",
      "          Predict: 0.0\n",
      "       Else (feature 15 > 0.005)\n",
      "        Predict: 1.0\n",
      "      Else (feature 29 > 0.185)\n",
      "       Predict: 0.0\n",
      "     Else (feature 45 > 0.015)\n",
      "      If (feature 4 <= 0.685)\n",
      "       Predict: 0.0\n",
      "      Else (feature 4 > 0.685)\n",
      "       Predict: 1.0\n",
      "    Else (feature 26 > 0.005)\n",
      "     Predict: 0.0\n",
      "  Else (feature 52 > 0.0525)\n",
      "   If (feature 24 <= 0.175)\n",
      "    If (feature 54 <= 2.339)\n",
      "     If (feature 6 <= 0.01)\n",
      "      If (feature 55 <= 7.5)\n",
      "       If (feature 18 <= 4.525)\n",
      "        Predict: 0.0\n",
      "       Else (feature 18 > 4.525)\n",
      "        If (feature 17 <= 0.005)\n",
      "         Predict: 0.0\n",
      "        Else (feature 17 > 0.005)\n",
      "         Predict: 1.0\n",
      "      Else (feature 55 > 7.5)\n",
      "       If (feature 20 <= 0.515)\n",
      "        If (feature 7 <= 0.005)\n",
      "         If (feature 9 <= 0.675)\n",
      "          If (feature 55 <= 15.5)\n",
      "           Predict: 0.0\n",
      "          Else (feature 55 > 15.5)\n",
      "           If (feature 11 <= 0.615)\n",
      "            Predict: 1.0\n",
      "           Else (feature 11 > 0.615)\n",
      "            Predict: 0.0\n",
      "         Else (feature 9 > 0.675)\n",
      "          Predict: 1.0\n",
      "        Else (feature 7 > 0.005)\n",
      "         If (feature 5 <= 0.005)\n",
      "          Predict: 1.0\n",
      "         Else (feature 5 > 0.005)\n",
      "          Predict: 0.0\n",
      "       Else (feature 20 > 0.515)\n",
      "        If (feature 1 <= 0.065)\n",
      "         If (feature 55 <= 9.5)\n",
      "          Predict: 0.0\n",
      "         Else (feature 55 > 9.5)\n",
      "          Predict: 1.0\n",
      "        Else (feature 1 > 0.065)\n",
      "         If (feature 44 <= 0.005)\n",
      "          If (feature 45 <= 0.015)\n",
      "           Predict: 1.0\n",
      "          Else (feature 45 > 0.015)\n",
      "           Predict: 0.0\n",
      "         Else (feature 44 > 0.005)\n",
      "          If (feature 4 <= 0.305)\n",
      "           Predict: 0.0\n",
      "          Else (feature 4 > 0.305)\n",
      "           Predict: 1.0\n",
      "     Else (feature 6 > 0.01)\n",
      "      If (feature 45 <= 0.015)\n",
      "       Predict: 1.0\n",
      "      Else (feature 45 > 0.015)\n",
      "       Predict: 0.0\n",
      "    Else (feature 54 > 2.339)\n",
      "     If (feature 45 <= 0.335)\n",
      "      If (feature 49 <= 0.646)\n",
      "       If (feature 54 <= 2.9939999999999998)\n",
      "        If (feature 10 <= 0.20500000000000002)\n",
      "         Predict: 1.0\n",
      "        Else (feature 10 > 0.20500000000000002)\n",
      "         If (feature 56 <= 187.5)\n",
      "          Predict: 1.0\n",
      "         Else (feature 56 > 187.5)\n",
      "          If (feature 49 <= 0.0365)\n",
      "           Predict: 0.0\n",
      "          Else (feature 49 > 0.0365)\n",
      "           Predict: 1.0\n",
      "       Else (feature 54 > 2.9939999999999998)\n",
      "        Predict: 1.0\n",
      "      Else (feature 49 > 0.646)\n",
      "       If (feature 54 <= 4.894)\n",
      "        Predict: 0.0\n",
      "       Else (feature 54 > 4.894)\n",
      "        Predict: 1.0\n",
      "     Else (feature 45 > 0.335)\n",
      "      If (feature 0 <= 0.135)\n",
      "       Predict: 0.0\n",
      "      Else (feature 0 > 0.135)\n",
      "       Predict: 1.0\n",
      "   Else (feature 24 > 0.175)\n",
      "    If (feature 51 <= 0.4395)\n",
      "     If (feature 53 <= 0.1175)\n",
      "      Predict: 0.0\n",
      "     Else (feature 53 > 0.1175)\n",
      "      If (feature 4 <= 1.0550000000000002)\n",
      "       Predict: 1.0\n",
      "      Else (feature 4 > 1.0550000000000002)\n",
      "       Predict: 0.0\n",
      "    Else (feature 51 > 0.4395)\n",
      "     Predict: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.toDebugString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar way to what was mentioned for scickit-learn, the features that allow making decisions in the top of the tree are more relevant for the decision problem: indirectly, decision trees allow feature selection.  \n",
    "\n",
    "A better visualisation of the tree in pyspark can be obtained by using, for example, [spark-tree-plotting](https://github.com/julioasotodv/spark-tree-plotting). The trick is to convert the spark tree to a JSON format. Once you have the JSON format, you can visualise it using [D3](https://d3js.org/) or you can transform from JSON to DOT and use graphviz as we did in scickit-learn. Are you able to generate a nicer visualisation of the tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Modify the program above to run the decision tree as part of a pipeline (see [Notebook 2](https://github.com/haipinglu/ScalableML/blob/master/Lab%202%20-%20RDD%2C%20DataFrame%2C%20ML%20pipeline%2C%20and%20parallelization.ipynb) for a refresher on pipelines). The pipeline model can be used to find the best set of parameters using cross validation. An example of a cross-validator can be found [here](http://spark.apache.org/docs/2.4.0/ml-tuning.html#cross-validation). In your case, make <tt>paramGrid</tt> contains different values for <tt>maxDepth</tt>, <tt>maxBins</tt> and <tt>impurity</tt> and find the best parameters and associated test error.\n",
    "\n",
    "### Question 3 \n",
    "\n",
    "Make the decision tree code a standalone program to run on HPC (see [Notebook 1](https://github.com/haipinglu/ScalableML/blob/master/Lab%201%20-%20Introduction%20to%20Spark%20and%20HPC.ipynb) for a refresher on how to run standalone programs in HPC or using the Notebook through Jupyter Hub).\n",
    "\n",
    "### Question 4 (optional)\n",
    "\n",
    "Create a decision tree classifier that runs on the [default of credit cards](http://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients) dataset. Several of the features in this dataset are categorical. Use the tools provided by PySpark (pyspark.ml.feature) for treating categorical variables. \n",
    "\n",
    "Note also that this dataset has a different format to the Spambase dataset above - you will need to convert from XLS format to, say, CSV, before using the data. You can use any available tool for this: for example, Excell has an export option, or there is a command line tool <tt>xls2csv</tt> available on Linux."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
