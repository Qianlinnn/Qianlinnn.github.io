{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced decision trees\n",
    "\n",
    "\n",
    "In this notebook we use Decision Trees for Regression and introduce the idea of ensemble methods through two popular implementations, namely, Random Forests and Gradient Boosting. The three algorithms, Decision Trees for Regression, Random Forests and Gradient Boosting have implementations in PySpark.\n",
    "\n",
    "## Decision trees for regression\n",
    "\n",
    "The main difference between Decision Tress for Classification and Decision Trees for Regression is in the impurity measure used. For regression, PySpark uses the variance of the target features as the impurity measure. \n",
    "\n",
    "We are going to use the [Wine Quality Dataset](http://archive.ics.uci.edu/ml/datasets/Wine+Quality) to illustrate the use of the **DecisionTreeRegression** class in PySpark. There are twelve input features corresponding to different attributes measured on wine samples (based on physicochemical tests). The target feature corresponds to a quality index that goes from zero to ten being zero a *very bad* wine and ten an *excellent* wine. The target feature was computed as the median score of three independent wine taster experts. More details on the dataset can be found in this [paper](https://www.sciencedirect.com/science/article/pii/S0167923609001377). \n",
    "\n",
    "We start by creating a <tt>SparkSession</tt> (unless you are running in a pyspark shell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import findspark\n",
    "# findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "spark = SparkSession.builder.master(\"local[2]\").appName(\"COM6012 Decision Trees Regression\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[fixed acidity: string, volatile acidity: string, citric acid: string, residual sugar: string, chlorides: string, free sulfur dioxide: string, total sulfur dioxide: string, density: string, pH: string, sulphates: string, alcohol: string, quality: string]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawdata = spark.read.csv('../Data/winequality-white.csv', sep=';', header='true')\n",
    "rawdata.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we use the parameter `sep=;` when loading the file, since the columns in the file are separated by `;` instead of the default `,`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fixed acidity: string (nullable = true)\n",
      " |-- volatile acidity: string (nullable = true)\n",
      " |-- citric acid: string (nullable = true)\n",
      " |-- residual sugar: string (nullable = true)\n",
      " |-- chlorides: string (nullable = true)\n",
      " |-- free sulfur dioxide: string (nullable = true)\n",
      " |-- total sulfur dioxide: string (nullable = true)\n",
      " |-- density: string (nullable = true)\n",
      " |-- pH: string (nullable = true)\n",
      " |-- sulphates: string (nullable = true)\n",
      " |-- alcohol: string (nullable = true)\n",
      " |-- quality: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rawdata.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now follow a very familiar procedure to get the dataset to a format that can be input to Spark MLlib, which consists of:\n",
    "1. transforming the data from type string to type double.\n",
    "2. to group the features into a type `SparseVector` or `DenseVector`.\n",
    "\n",
    "We first start transforming the data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemaNames = rawdata.schema.names\n",
    "ncolumns = len(rawdata.columns)\n",
    "from pyspark.sql.types import DoubleType\n",
    "for i in range(ncolumns):\n",
    "    rawdata = rawdata.withColumn(schemaNames[i], rawdata[schemaNames[i]].cast(DoubleType())) # cast 变数据类型\n",
    "rawdata = rawdata.withColumnRenamed('quality', 'labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we used the [<tt>withColumnRenamed</tt>](http://spark.apache.org/docs/2.3.2/api/python/pyspark.sql.html?highlight=withcolumn#pyspark.sql.DataFrame.withColumnRenamed) method to rename the name of the target feature from 'quality' to 'label'. We can now create a DataFrame with the data that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fixed acidity: double (nullable = true)\n",
      " |-- volatile acidity: double (nullable = true)\n",
      " |-- citric acid: double (nullable = true)\n",
      " |-- residual sugar: double (nullable = true)\n",
      " |-- chlorides: double (nullable = true)\n",
      " |-- free sulfur dioxide: double (nullable = true)\n",
      " |-- total sulfur dioxide: double (nullable = true)\n",
      " |-- density: double (nullable = true)\n",
      " |-- pH: double (nullable = true)\n",
      " |-- sulphates: double (nullable = true)\n",
      " |-- alcohol: double (nullable = true)\n",
      " |-- labels: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rawdata.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|labels|\n",
      "+------+\n",
      "|   6.0|\n",
      "|   6.0|\n",
      "|   6.0|\n",
      "|   6.0|\n",
      "|   6.0|\n",
      "|   6.0|\n",
      "|   6.0|\n",
      "|   6.0|\n",
      "|   6.0|\n",
      "|   6.0|\n",
      "|   5.0|\n",
      "|   5.0|\n",
      "|   5.0|\n",
      "|   7.0|\n",
      "|   5.0|\n",
      "|   7.0|\n",
      "|   6.0|\n",
      "|   8.0|\n",
      "|   6.0|\n",
      "|   5.0|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rawdata.select('labels').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the [<tt>VectorAssembler</tt>](http://spark.apache.org/docs/2.3.2/api/python/pyspark.ml.html?highlight=vectorassembler#pyspark.ml.feature.VectorAssembler) tool to concatenate all the features in a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols = schemaNames[0:ncolumns-1], outputCol = 'features') \n",
    "raw_plus_vector = assembler.transform(rawdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = raw_plus_vector.select('features','labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- labels: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data in the correct format, we can proceed to build the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingData, testData) = data.randomSplit([0.7, 0.3], 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [DecisionTreeRegressor](http://spark.apache.org/docs/2.3.2/api/python/pyspark.ml.html?highlight=decisiontreeregress#pyspark.ml.regression.DecisionTreeRegressor) implemented in PySpark has several parameters to tune. Some of them are\n",
    "\n",
    "> **maxDepth**: it corresponds to the maximum depth of the tree. The default is 5.<p>\n",
    "**maxBins**: it determines how many bins should be created from continuous features. The default is 32.<p>\n",
    "    **impurity**: for regression the only supported impurity option is variance.<p>\n",
    "        **minInfoGain**: it determines the minimum information gain that will be used for a split. The default is zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "dt = DecisionTreeRegressor(labelCol=\"labels\", featuresCol=\"features\", maxDepth=5)\n",
    "model = dt.fit(trainingData)\n",
    "predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally use the [RegressionEvaluator](http://spark.apache.org/docs/2.3.2/api/python/pyspark.ml.html?highlight=regressionevaluator#pyspark.ml.evaluation.RegressionEvaluator) tool to assess the rmse on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 0.762375 \n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator\\\n",
    "      (labelCol=\"labels\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"RMSE = %g \" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "As we did for the Decision Trees for Classification, it is possible to use the [featureImportances](http://spark.apache.org/docs/2.3.2/api/python/pyspark.ml.html?highlight=featureimportances#pyspark.ml.regression.DecisionTreeRegressionModel.featureImportances) method to study the relative importance of each features. Use the *featureImportances* in the model above and indicate the three most relevant features.\n",
    "\n",
    "### Question 2\n",
    "\n",
    "Write a pipeline that includes a parameter grid that allows you to find the best parameter configuration for the parameters *maxDepth* and *maxBins*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble learning\n",
    "\n",
    "In machine learning, we use the term ensemble model to refer to a predictive model that is a composition of several other predictive models. For example, for a classification problem, we can have an ensemble of three classifiers, where the first of them is a Naive Bayes classifier, the second one is a logistic regressor and the third one is a decision tree classifier. We can train all classifiers with the same training data and then, at test time, predictions can be done using majority voting. \n",
    "\n",
    "Ensemble methods are very popular since they usually show higher performance when compared to simple classifiers. In fact, gradient boosting trees are the most popular method in [**Kaggle**](https://www.kaggle.com/), a platform that hosts data science competitions. The top entry in the [**Netflix Prize**](https://en.wikipedia.org/wiki/Netflix_Prize) Competition, one of the most famous data science competitions, was based on an ensemble predictive model. \n",
    "\n",
    "The most commmon ensemble methods use decision trees as the members of the ensemble. PySpark implemenst two types of Tree Ensembles, random forests and gradient boosting. The main difference between both methods is the way in which they combine the different trees that compose the ensemble.\n",
    "\n",
    "### Random Forests\n",
    "\n",
    "The variant of Random Forests implemented in Apache Spark is also known as bagging or boostrap aggregating. The tree ensemble in random forests is built by training individual decision trees on different subsets of the training data and using a subset of the available features. For classification, the prediction is done by majority voting among the individual trees. For regression, the prediction is the average of the individual predictions of each tree. For more details on the PySpark implmentation see [here](http://spark.apache.org/docs/2.3.2/mllib-ensembles.html#random-forests). \n",
    "\n",
    "Besides the parameters that we already mentioned for the [DecisionTreeClassifier](http://spark.apache.org/docs/2.3.2/api/python/pyspark.ml.html?highlight=decisiontreeclassifier#pyspark.ml.classification.DecisionTreeClassifier) and the [DecisionTreeRegressor](http://spark.apache.org/docs/2.3.2/api/python/pyspark.ml.html?highlight=decisiontreeregress#pyspark.ml.regression.DecisionTreeRegressor), the [RandomForestClassifier](http://spark.apache.org/docs/2.3.2/api/python/pyspark.ml.html?highlight=randomforestclassifier#pyspark.ml.classification.RandomForestClassifier) and the [RandomForestRegressor](http://spark.apache.org/docs/2.3.2/api/python/pyspark.ml.html?highlight=randomforestregressor#pyspark.ml.regression.RandomForestRegressor) in PySpark require three additional parameters:\n",
    "> **numTrees** the total number of trees to train<p>\n",
    "**featureSubsetStrategy** number of features to use as candidates for splitting at each tree node. Options include all, onethird, sqrt, log2, [1-n]<p>\n",
    "    **subsamplingRate**: size of the dataset used for training each tree in the forest, as a fraction of the size of the original dataset. \n",
    "\n",
    "Let us use RandomForestRegressor on the wine quality dataset from above and evaluate the performance on the same data set partition that we had before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 0.726584 \n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor  # \n",
    "rfr = RandomForestRegressor(labelCol=\"labels\", featuresCol=\"features\", maxDepth=5, numTrees=3) # 选取label（y）和feature\n",
    "model = rfr.fit(trainingData)#  \n",
    "predictions = model.transform(testData)# 做出预测\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator\\ \n",
    "      (labelCol=\"labels\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"RMSE = %g \" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use [featuresImportance](http://spark.apache.org/docs/2.3.2/api/python/pyspark.ml.html?highlight=featureimportances#pyspark.ml.regression.RandomForestRegressionModel.featureImportances) for the RandomForestRegressor and RandomForestClassifier models. How are the feature importances computed in this case? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Write a pipeline that includes a parameter grid that allows you to find the best parameter configuration for the parameters *maxDepth*, *maxBins*, *numTrees*, *featureSubsetStrategy* and *subsamplingRate*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "\n",
    "In [Gradient Boosting](https://en.wikipedia.org/wiki/Gradient_boosting) or [Gradient-boosted trees](https://en.wikipedia.org/wiki/Gradient_boosting#Gradient_tree_boosting) (GBT), each tree in the ensemble is trained sequentially: the first tree is trained as usual using the training data, the second tree is trained on the residuals between the predictions of the first tree and the labels of the training data, the third tree is trained on the residuals of the predictions of the second tree, etc. The predictions of the ensemble will be the sum of the predictions of each individual tree. The type of residuals are related to the loss function that wants to be minimised. In the PySpark implementations of Gradient-Boosted trees, the loss function for binary classification is the Log-Loss function and the loss function for regression is either the squared error or the absolute error. For details, follow this [link](http://spark.apache.org/docs/2.3.2/mllib-ensembles.html#gradient-boosted-trees-gbts).  \n",
    "\n",
    "PySpark uses the classes [GBTRegressor](http://spark.apache.org/docs/2.3.2/api/python/pyspark.ml.html?highlight=gradient%20boosting#pyspark.ml.regression.GBTRegressor) for the implementation of Gradient-Boosted trees for regression and [GBTClassifier](http://spark.apache.org/docs/2.3.2/api/python/pyspark.ml.html?highlight=gbtclassifier#pyspark.ml.classification.GBTClassifier) for the implementation of Gradient-Boosted trees for binary classification. As of PySpark version 2.3.2 GBT have not been implemented for multiclass classification.\n",
    "\n",
    "Besides the parameters that can be specified for Decision Trees, both classes share the additional following parameters\n",
    "\n",
    ">**lossType** type of loss function. Options are \"squared\" and \"absolute\" for regression and \"logistic\" for classification. <p>\n",
    "    **maxIter** number of trees in the ensemble. Each iteration produces one tree.<p>\n",
    "    **stepSize** also known as the learning rate, it is used for shrinking the contribution of each tree in the sequence. The default is 0.1<p>\n",
    "    **subsamplingRate** as it was the case for Random Forest, this parameter is used for specifying the fraction of the training data used for learning each decision tree.    \n",
    "\n",
    "We will now use the GBTRegressor on the wine quality dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 0.747853 \n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "gbtr = GBTRegressor(labelCol=\"labels\", featuresCol=\"features\", maxDepth=5, maxIter=5, lossType='squared')\n",
    "model = gbtr.fit(trainingData)\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator \\\n",
    "      (labelCol=\"labels\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"RMSE = %g \" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Write and run an HPC standalone program using random forest regression on the [Physical Activity Monitoring](http://archive.ics.uci.edu/ml/datasets/PAMAP2+Physical+Activity+Monitoring) dataset, methodically experimenting with the parameters *maxDepth*, *numTrees* and *subsamplingRate*. Obtain the timing for the experiment. Note that the <tt>physical activity monitoring</tt> dataset contains <tt>NaN</tt> (not a number) values when values are missing - you should try dealing with this in two ways\n",
    "\n",
    "1. Drop lines containing <tt>NaN</tt>\n",
    "2. Replace <tt>NaN</tt> with the average value from that column. For this, you can use the [Imputer](http://spark.apache.org/docs/2.3.2/ml-features.html#imputer) transformer available in <tt>pyspark.ml.feature</tt> \n",
    "\n",
    "Run experiments with both options."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
