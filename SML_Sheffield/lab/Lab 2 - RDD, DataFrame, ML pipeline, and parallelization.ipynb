{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COM6012 Scalable Machine Learning 2019 - Haiping Lu\n",
    "# Lab 2 - RDD, DataFrame, ML pipeline, & parallelization\n",
    "\n",
    "## Objectives\n",
    "\n",
    "* Task 1: To finish in the lab session. **Essential**\n",
    "* Task 2: To finish in the lab session. **Essential**\n",
    "* Task 3: To finish in the lab session. **Essential**\n",
    "* Task 4: To explore by yourself before the next session. **Optional but recommended**\n",
    "\n",
    "**Suggested reading**: \n",
    "* Chapters 5 and 6, and especially **Section 8.1** (of Chapter 8)  of [PySpark tutorial](https://runawayhorse001.github.io/LearningApacheSpark/pyspark.pdf)\n",
    "* [RDD Programming Guide](https://spark.apache.org/docs/2.3.2/rdd-programming-guide.html): Most are useful to know in this module.\n",
    "* [Spark SQL, DataFrames and Datasets Guide](https://spark.apache.org/docs/2.3.2/sql-programming-guide.html): `Overview` and `Getting Started` recommended (skipping those without Python example).\n",
    "* [Machine Learning Library (MLlib) Guide](https://spark.apache.org/docs/2.3.2/ml-guide.html)\n",
    "* [ML Pipelines](https://spark.apache.org/docs/2.3.2/ml-pipeline.html)\n",
    "* [Apache Spark Examples](https://spark.apache.org/examples.html)\n",
    "* [Basic Statistics - DataFrame API](https://spark.apache.org/docs/2.3.2/ml-statistics.html)\n",
    "* [Basic Statistics - RDD API](https://spark.apache.org/docs/2.3.2/mllib-statistics.html): much **richer**\n",
    "\n",
    "**Something compact?**\n",
    "* [Cheat sheet PySpark Python](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_Cheat_Sheet_Python.pdf): **Highly recommended. Very handy.** \n",
    "* [Cheat sheet PySpark SQL Python](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf): **Highly recommended. Very handy.** \n",
    "* [Cheat sheet for PySpark (2 page version)](https://github.com/runawayhorse001/CheatSheet/blob/master/cheatSheet_pyspark.pdf): **Highly recommended. Very handy.** \n",
    "* If you find any good stuff, please share with me and all. Thanks.\n",
    "\n",
    "**Tip**: Try to use as much DataFrame APIs as possible by referring to the [Pyspark API documentation](https://spark.apache.org/docs/2.3.2/api/python/index.html). When you try to program something, try to search whether there is a function in the API already.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If running this notebook on HPC via [Jupyter Hub](https://jupyter-sharc.shef.ac.uk/), we need to run the following cell. If we are running this notebook on our local machine, skip the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] 系统找不到指定的文件。",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-fd3f3b75fbac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'/usr/bin/modulecmd'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'load'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'apps/java/jdk1.8.0_102/binary'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'PYSPARK_PYTHON'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'HOME'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/.conda/envs/jupyter-spark/bin/python'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-fd3f3b75fbac>\u001b[0m in \u001b[0;36mmodule\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'/usr/bin/modulecmd'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'load'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'apps/java/jdk1.8.0_102/binary'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[0;32m    773\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    774\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 775\u001b[1;33m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[0;32m    776\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m             \u001b[1;31m# Cleanup if the child failed starting.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1176\u001b[0m                                          \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m                                          \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcwd\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcwd\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m                                          startupinfo)\n\u001b[0m\u001b[0;32m   1179\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m                 \u001b[1;31m# Child is launched. Close the parent's copy of those pipe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] 系统找不到指定的文件。"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "def module(*args):        \n",
    "    if isinstance(args[0], list):        \n",
    "        args = args[0]        \n",
    "    else:        \n",
    "        args = list(args)        \n",
    "    (output, error) = subprocess.Popen(['/usr/bin/modulecmd', 'python'] + args, stdout=subprocess.PIPE).communicate()\n",
    "    exec(output)    \n",
    "module('load', 'apps/java/jdk1.8.0_102/binary')    \n",
    "os.environ['PYSPARK_PYTHON'] = os.environ['HOME'] + '/.conda/envs/jupyter-spark/bin/python'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unless you are running in a pyspark shell, you need to first create `spark` and `sc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import findspark\n",
    "#findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .appName(\"COM6012 Lab 2: RDD, DataFrame, ML pipeline, parallelization\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also copy-paste into a shell or download the code as a `.py` file to run as standalone program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RDD and Shared Variables\n",
    "\n",
    "\n",
    "Spark allows for parallel operations in a program to be executed on a cluster.\n",
    "* **Main abstraction**: **resilient distributed dataset (RDD)** is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel.\n",
    "* **Second abstraction**: **shared variables** that can be shared across tasks, or between tasks and the driver program. Two types:\n",
    "    * *Broadcast variables*, which can be used to cache a value in memory on all nodes\n",
    "    * *Accumulators*, which are variables that are only “added” to, such as counters and sums.\n",
    "\n",
    "#### Parallelized collections\n",
    "\n",
    "* Parallelized collections are created by calling `SparkContext`’s `parallelize` method on an existing iterable or collection in your driver program. The elements of the collection are copied to form a distributed dataset that can be operated on in parallel. For example, here is how to create a parallelized collection holding the numbers 1 to 5:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "rddData = sc.parallelize(data) #创建RDD数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddData.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the number of *partitions* can be set manually, by passing parallelize a second argument to the sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[6] at readRDDFromFile at PythonRDD.scala:247"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(data, 10) #set partitions to 10 #指定将数据集分成10块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark tries to set the number of partitions automatically based on the cluster, the rule being 2-4 partitions for every CPU in the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\pi$ Estimation\n",
    "Spark can also be used for compute-intensive tasks. This code estimates $\\pi$ by \"throwing darts\" at a circle. We pick random points in the unit square ((0, 0) to (1,1)) and see how many fall in the unit circle. The fraction should be $\\pi / 4$, so we use this to get our estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7885\n",
      "Pi is roughly 3.154\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "NUM_SAMPLES = 10000  # 总共的飞镖数\n",
    "def inside(p): \n",
    "    x, y = random.random(), random.random() # 生成0-1的随机小数\n",
    "    return x*x + y*y < 1 # 将x，y限制在圆里，又由于是正方形，所以是四分之一圆\n",
    "count = sc.parallelize(range(0, NUM_SAMPLES)).filter(inside).count() # 落在圆的飞镖数，第一个参数表示多少个，第二个参数是条件，count是统计总个数\n",
    "print(count)\n",
    "pi = 4 * count / NUM_SAMPLES      \n",
    "print(\"Pi is roughly\", pi)\n",
    "# 落在圆面积里的飞镖比正方形的飞镖的值为pi/4 ,即A/(A+B) = pi/4 所以pi = 4*A/(A+B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change `NUM_SAMPLES` to see the difference in precision and time cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding closures\n",
    "\n",
    "There is a difference between running in local and cluster mode; the main difference is variable values. Note that you may get unexpected behaviour due to assumptions about variable values being updated / not being updated!\n",
    "\n",
    "#### Broadcast variables\n",
    "\n",
    "To avoid creating a copy of a variable for each task, an accessible (read-only!) variable can be kept on each machine - this is useful for particularly large datasets which may be needed for multiple tasks. The data broadcasted this way is cached in serialized form and deserialized before running each task.\n",
    "\n",
    "Broadcast variables are created from a variable $v$ by calling SparkContext.broadcast(v). The broadcast variable is a wrapper around $v$, and its value can be accessed by calling the *value* method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcastVar = sc.broadcast([1, 2, 3])\n",
    "broadcastVar.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accumulators\n",
    "\n",
    "[Accumulators](https://spark.apache.org/docs/2.3.2/api/java/org/apache/spark/Accumulator.html) are variables that are only “added” to through an associative and commutative operation and can therefore be efficiently supported in parallel. They can be used to implement counters (as in <tt>MapReduce</tt>) or sums. 可以实现计数器和求和\n",
    "\n",
    "You can create a numeric accumulator by calling *SparkContext.longAccumulator()* or *SparkContext.doubleAccumulator()* to accumulate either Long or Double values (it is possible for users to create their own accumulators of different type), and the accumulator is created with an initial value. Cluster tasks can then add to it using <tt>add</tt>. However, they cannot read its value - that can only be read using <tt>value</tt> by the driver program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accumulator<id=0, value=0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum = sc.accumulator(0)\n",
    "accum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1, 2, 3, 4]).foreach(lambda x: accum.add(x))\n",
    "accum.value # 计算累加 初始值是0， 1+2+3+4 = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that accumulators are only guaranteed to update the value of a variable once for updates within *actions*, within lazy transforms (such as *map*) accumulator updates are not guaranteed to be executed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[17] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "accum = sc.accumulator(5)\n",
    "def g(x):\n",
    "    accum.add(x)\n",
    "\n",
    "rddData.map(g) # 创建了一个map g\n",
    "# Here, accum is still 0 because no actions have caused the `map` to be computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "g(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accumulator<id=1, value=10>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "g(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accumulator<id=1, value=35>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Frame\n",
    "\n",
    "Along with the introduction of <tt>SparkSession</tt>, the <tt>resilient distributed dataset</tt> (RDD) was replaced by [dataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset). Again, these are objects which can be worked on in parallel. The available operations are:\n",
    "\n",
    "- **transformations**: produce new datasets\n",
    "- **actions**: computations which return results\n",
    "\n",
    "We will start with creating dataframes and datasets, showing how we can print their contents. We create a dataframe in the cell below and print out some info (we can also modify the output before printing):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From RDD to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(1,2,3),(4,5,6),(7,8,9)]) # 创建rdd\n",
    "df = rdd.toDF([\"a\",\"b\",\"c\"]) #将rdd转成dataframe 并加入了三个column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[18] at readRDDFromFile at PythonRDD.scala:247"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: bigint, c: bigint]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  a|  b|  c|\n",
      "+---+---+---+\n",
      "|  1|  2|  3|\n",
      "|  4|  5|  6|\n",
      "|  7|  8|  9|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: long (nullable = true)\n",
      " |-- c: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema() #打印数据类型 并且是否还有标签"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get RDD from DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[29] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2=df.rdd # 从dataframe转化成RDD\n",
    "rdd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(a=1, b=2, c=3), Row(a=4, b=5, c=6), Row(a=7, b=8, c=9)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.collect() # 显示rdd2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data from CSV file. This data is from a [classic book on statistical learning](http://www-bcf.usc.edu/~gareth/ISL/data.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.load(\"Data/Advertising.csv\",\n",
    "                     format=\"csv\", inferSchema=\"true\", header=\"true\")  # 载入数据 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+---------+-----+\n",
      "|_c0|   TV|radio|newspaper|sales|\n",
      "+---+-----+-----+---------+-----+\n",
      "|  1|230.1| 37.8|     69.2| 22.1|\n",
      "|  2| 44.5| 39.3|     45.1| 10.4|\n",
      "|  3| 17.2| 45.9|     69.3|  9.3|\n",
      "|  4|151.5| 41.3|     58.5| 18.5|\n",
      "|  5|180.8| 10.8|     58.4| 12.9|\n",
      "|  6|  8.7| 48.9|     75.0|  7.2|\n",
      "|  7| 57.5| 32.8|     23.5| 11.8|\n",
      "+---+-----+-----+---------+-----+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- TV: double (nullable = true)\n",
      " |-- radio: double (nullable = true)\n",
      " |-- newspaper: double (nullable = true)\n",
      " |-- sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema() #schema是csv数据里的列表名"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us remove the first column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df.drop('_c0')#drop删除第一列的列名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- TV: double (nullable = true)\n",
      " |-- radio: double (nullable = true)\n",
      " |-- newspaper: double (nullable = true)\n",
      " |-- sales: double (nullable = true)\n",
      "\n",
      "+-----+-----+---------+-----+\n",
      "|   TV|radio|newspaper|sales|\n",
      "+-----+-----+---------+-----+\n",
      "|230.1| 37.8|     69.2| 22.1|\n",
      "| 44.5| 39.3|     45.1| 10.4|\n",
      "| 17.2| 45.9|     69.3|  9.3|\n",
      "|151.5| 41.3|     58.5| 18.5|\n",
      "|180.8| 10.8|     58.4| 12.9|\n",
      "|  8.7| 48.9|     75.0|  7.2|\n",
      "| 57.5| 32.8|     23.5| 11.8|\n",
      "|120.2| 19.6|     11.6| 13.2|\n",
      "|  8.6|  2.1|      1.0|  4.8|\n",
      "|199.8|  2.6|     21.2| 10.6|\n",
      "| 66.1|  5.8|     24.2|  8.6|\n",
      "|214.7| 24.0|      4.0| 17.4|\n",
      "| 23.8| 35.1|     65.9|  9.2|\n",
      "| 97.5|  7.6|      7.2|  9.7|\n",
      "|204.1| 32.9|     46.0| 19.0|\n",
      "|195.4| 47.7|     52.9| 22.4|\n",
      "| 67.8| 36.6|    114.0| 12.5|\n",
      "|281.4| 39.6|     55.8| 24.4|\n",
      "| 69.2| 20.5|     18.3| 11.3|\n",
      "|147.3| 23.9|     19.1| 14.6|\n",
      "+-----+-----+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get **summary statistics** for numerical columns using **`.describe().show()`**, very handy to inspect your (big) data for understanding/debugging.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+------------------+------------------+\n",
      "|summary|               TV|             radio|         newspaper|             sales|\n",
      "+-------+-----------------+------------------+------------------+------------------+\n",
      "|  count|              200|               200|               200|               200|\n",
      "|   mean|         147.0425|23.264000000000024|30.553999999999995|14.022500000000003|\n",
      "| stddev|85.85423631490805|14.846809176168728| 21.77862083852283| 5.217456565710477|\n",
      "|    min|              0.7|               0.0|               0.3|               1.6|\n",
      "|    max|            296.4|              49.6|             114.0|              27.0|\n",
      "+-------+-----------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.describe().show()  #用列把数据总结展示出来"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Machine Learning Libary and Pipelines\n",
    "\n",
    "[MLlib](https://spark.apache.org/docs/2.3.2/ml-guide.html) is Spark’s machine learning (ML) library. It provides:\n",
    "\n",
    "- *ML Algorithms*: common learning algorithms such as classification, regression, clustering, and collaborative filtering\n",
    "- *Featurization*: feature extraction, transformation, dimensionality reduction, and selection\n",
    "- *Pipelines*: tools for constructing, evaluating, and tuning ML Pipelines\n",
    "- *Persistence*: saving and load algorithms, models, and Pipelines\n",
    "- *Utilities*: linear algebra, statistics, data handling, etc.\n",
    "\n",
    "<tt>MLlib</tt> allows easy combination of numerous algorithms into a single pipeline using standardized APIs for machine learning algorithms. The key concepts are:\n",
    "\n",
    "- **Dataframe**. Dataframes can hold a variety of data types.\n",
    "- **Transformer**. Transforms one dataframe into another.\n",
    "- **Estimator**. Algorithm which can be fit on a DataFrame to produce a Transformer.\n",
    "- **Pipeline**. A Pipeline chains multiple Transformers and Estimators together to specify an ML workflow.\n",
    "- **Parameter**. Transformers and Estimators share a common API for specifying parameters.\n",
    "\n",
    "A list of some of the available ML features is available [here](http://spark.apache.org/docs/2.3.2/ml-features.html).\n",
    "\n",
    "**Clarification on whether Estimator is a transformer**. See [Estimators](https://spark.apache.org/docs/2.3.2/ml-pipeline.html#estimators)\n",
    "> An Estimator abstracts the concept of a learning algorithm or any algorithm that fits or trains on data. Technically, an Estimator implements a method fit(), which accepts a DataFrame and produces a Model, which is a Transformer. For example, a learning algorithm such as LogisticRegression is an Estimator, and calling fit() trains a LogisticRegressionModel, which is a Model and hence a **Transformer**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Example\n",
    "The example below is based on **Section 8.1** of [PySpark tutorial](https://runawayhorse001.github.io/LearningApacheSpark/pyspark.pdf). \n",
    "\n",
    "#### Convert the data to dense vector (features and label)\n",
    "\n",
    "Let us convert the above data in CSV format to a typical (feature, label) pair for supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transData(data):\n",
    "    return data.rdd.map(lambda r: [Vectors.dense(r[:-1]),r[-1]]).toDF(['features','label'])# 后面的参数命名column\n",
    "# Vectors.dense([r:[-1],r[-1]])其中r是传入的数据，即data， 将数据切割成两块，一个是从第一列到倒数第二列，另一块是最后一列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|         features|label|\n",
      "+-----------------+-----+\n",
      "|[230.1,37.8,69.2]| 22.1|\n",
      "| [44.5,39.3,45.1]| 10.4|\n",
      "| [17.2,45.9,69.3]|  9.3|\n",
      "|[151.5,41.3,58.5]| 18.5|\n",
      "|[180.8,10.8,58.4]| 12.9|\n",
      "|  [8.7,48.9,75.0]|  7.2|\n",
      "| [57.5,32.8,23.5]| 11.8|\n",
      "|[120.2,19.6,11.6]| 13.2|\n",
      "|    [8.6,2.1,1.0]|  4.8|\n",
      "| [199.8,2.6,21.2]| 10.6|\n",
      "|  [66.1,5.8,24.2]|  8.6|\n",
      "| [214.7,24.0,4.0]| 17.4|\n",
      "| [23.8,35.1,65.9]|  9.2|\n",
      "|   [97.5,7.6,7.2]|  9.7|\n",
      "|[204.1,32.9,46.0]| 19.0|\n",
      "|[195.4,47.7,52.9]| 22.4|\n",
      "|[67.8,36.6,114.0]| 12.5|\n",
      "|[281.4,39.6,55.8]| 24.4|\n",
      "| [69.2,20.5,18.3]| 11.3|\n",
      "|[147.3,23.9,19.1]| 14.6|\n",
      "|[218.4,27.7,53.4]| 18.0|\n",
      "| [237.4,5.1,23.5]| 12.5|\n",
      "| [13.2,15.9,49.6]|  5.6|\n",
      "|[228.3,16.9,26.2]| 15.5|\n",
      "| [62.3,12.6,18.3]|  9.7|\n",
      "| [262.9,3.5,19.5]| 12.0|\n",
      "|[142.9,29.3,12.6]| 15.0|\n",
      "|[240.1,16.7,22.9]| 15.9|\n",
      "|[248.8,27.1,22.9]| 18.9|\n",
      "| [70.6,16.0,40.8]| 10.5|\n",
      "|[292.9,28.3,43.2]| 21.4|\n",
      "|[112.9,17.4,38.6]| 11.9|\n",
      "|  [97.2,1.5,30.0]|  9.6|\n",
      "| [265.6,20.0,0.3]| 17.4|\n",
      "|   [95.7,1.4,7.4]|  9.5|\n",
      "|  [290.7,4.1,8.5]| 12.8|\n",
      "| [266.9,43.8,5.0]| 25.4|\n",
      "| [74.7,49.4,45.7]| 14.7|\n",
      "| [43.1,26.7,35.1]| 10.1|\n",
      "|[228.0,37.7,32.0]| 21.5|\n",
      "|[202.5,22.3,31.6]| 16.6|\n",
      "|[177.0,33.4,38.7]| 17.1|\n",
      "| [293.6,27.7,1.8]| 20.7|\n",
      "| [206.9,8.4,26.4]| 12.9|\n",
      "| [25.1,25.7,43.3]|  8.5|\n",
      "|[175.1,22.5,31.5]| 14.9|\n",
      "|  [89.7,9.9,35.7]| 10.6|\n",
      "|[239.9,41.5,18.5]| 23.2|\n",
      "|[227.2,15.8,49.9]| 14.8|\n",
      "| [66.9,11.7,36.8]|  9.7|\n",
      "| [199.8,3.1,34.6]| 11.4|\n",
      "|  [100.4,9.6,3.6]| 10.7|\n",
      "|[216.4,41.7,39.6]| 22.6|\n",
      "|[182.6,46.2,58.7]| 21.2|\n",
      "|[262.7,28.8,15.9]| 20.2|\n",
      "|[198.9,49.4,60.0]| 23.7|\n",
      "|  [7.3,28.1,41.4]|  5.5|\n",
      "|[136.2,19.2,16.6]| 13.2|\n",
      "|[210.8,49.6,37.7]| 23.8|\n",
      "| [210.7,29.5,9.3]| 18.4|\n",
      "|  [53.5,2.0,21.4]|  8.1|\n",
      "|[261.3,42.7,54.7]| 24.2|\n",
      "|[239.3,15.5,27.3]| 15.7|\n",
      "| [102.7,29.6,8.4]| 14.0|\n",
      "|[131.1,42.8,28.9]| 18.0|\n",
      "|   [69.0,9.3,0.9]|  9.3|\n",
      "|  [31.5,24.6,2.2]|  9.5|\n",
      "|[139.3,14.5,10.2]| 13.4|\n",
      "|[237.4,27.5,11.0]| 18.9|\n",
      "|[216.8,43.9,27.2]| 22.3|\n",
      "|[199.1,30.6,38.7]| 18.3|\n",
      "|[109.8,14.3,31.7]| 12.4|\n",
      "| [26.8,33.0,19.3]|  8.8|\n",
      "| [129.4,5.7,31.3]| 11.0|\n",
      "|[213.4,24.6,13.1]| 17.0|\n",
      "| [16.9,43.7,89.4]|  8.7|\n",
      "|  [27.5,1.6,20.7]|  6.9|\n",
      "|[120.5,28.5,14.2]| 14.2|\n",
      "|   [5.4,29.9,9.4]|  5.3|\n",
      "| [116.0,7.7,23.1]| 11.0|\n",
      "| [76.4,26.7,22.3]| 11.8|\n",
      "| [239.8,4.1,36.9]| 12.3|\n",
      "| [75.3,20.3,32.5]| 11.3|\n",
      "| [68.4,44.5,35.6]| 13.6|\n",
      "|[213.5,43.0,33.8]| 21.7|\n",
      "|[193.2,18.4,65.7]| 15.2|\n",
      "| [76.3,27.5,16.0]| 12.0|\n",
      "|[110.7,40.6,63.2]| 16.0|\n",
      "| [88.3,25.5,73.4]| 12.9|\n",
      "|[109.8,47.8,51.4]| 16.7|\n",
      "|  [134.3,4.9,9.3]| 11.2|\n",
      "|  [28.6,1.5,33.0]|  7.3|\n",
      "|[217.7,33.5,59.0]| 19.4|\n",
      "|[250.9,36.5,72.3]| 22.2|\n",
      "|[107.4,14.0,10.9]| 11.5|\n",
      "|[163.3,31.6,52.9]| 16.9|\n",
      "|  [197.6,3.5,5.9]| 11.7|\n",
      "|[184.9,21.0,22.0]| 15.5|\n",
      "|[289.7,42.3,51.2]| 25.4|\n",
      "|[135.2,41.7,45.9]| 17.2|\n",
      "+-----------------+-----+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed= transData(df2)# df2是一个dataframe，里面存储的是dense vector\n",
    "transformed.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels here are real numbers and this is a **regression** problem. For **classification** problem, you may need to transform labels (e.g., *disease*,*healthy*) to indices with a featureIndexer in Step 5, **Section 8.1** of [PySpark tutorial]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data into training and test sets (40% held out for testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingData, testData) = transformed.randomSplit([0.6, 0.4])#将数据按照6：4随机分为训练集和测试集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your train and test data as follows. I agree with the author Wenqiang that **it is always to good to keep tracking your data during prototype phase**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|        features|label|\n",
      "+----------------+-----+\n",
      "|  [0.7,39.6,8.7]|  1.6|\n",
      "|  [5.4,29.9,9.4]|  5.3|\n",
      "| [7.3,28.1,41.4]|  5.5|\n",
      "| [8.7,48.9,75.0]|  7.2|\n",
      "|[13.2,15.9,49.6]|  5.6|\n",
      "+----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingData.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|        features|label|\n",
      "+----------------+-----+\n",
      "|   [8.6,2.1,1.0]|  4.8|\n",
      "| [8.7,48.9,75.0]|  7.2|\n",
      "|[13.2,15.9,49.6]|  5.6|\n",
      "|[16.9,43.7,89.4]|  8.7|\n",
      "|[17.2,45.9,69.3]|  9.3|\n",
      "+----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testData.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Fit a Linear Regression Model and Perform prediction\n",
    "More details on parameters can be found in the [Python API documentation](https://spark.apache.org/docs/2.3.2/api/python/pyspark.ml.html#pyspark.ml.regression.LinearRegression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression() #创建线性回归变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel = lr.fit(trainingData) #用训练数据直接出模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lrModel.transform(testData)# 直接做预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+------------------+\n",
      "|        features|label|        prediction|\n",
      "+----------------+-----+------------------+\n",
      "|  [4.1,11.6,5.7]|  3.2| 5.536963170706832|\n",
      "| [7.8,38.9,50.6]|  6.6|10.466703454241049|\n",
      "|  [8.4,27.2,2.1]|  5.7| 8.579489800720614|\n",
      "|   [8.6,2.1,1.0]|  4.8|  4.03466338431621|\n",
      "|[11.7,36.9,45.2]|  7.3|10.302559250941938|\n",
      "+----------------+-----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\",predictionCol=\"prediction\",metricName=\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 1.65288\n"
     ]
    }
   ],
   "source": [
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Pipeline Example\n",
    "\n",
    "This example is from the [ML Pipeline API](https://spark.apache.org/docs/2.3.2/ml-pipeline.html), with additional explanations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directly create DataFrame (for illustration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])  # 直接创建dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+-----+\n",
      "| id|            text|label|\n",
      "+---+----------------+-----+\n",
      "|  0| a b c d e spark|  1.0|\n",
      "|  1|             b d|  0.0|\n",
      "|  2|     spark f g h|  1.0|\n",
      "|  3|hadoop mapreduce|  0.0|\n",
      "+---+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model fitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct test documents (data), which are unlabeled (id, text) tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"spark hadoop spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "| id|              text|\n",
      "+---+------------------+\n",
      "|  4|       spark i j k|\n",
      "|  5|             l m n|\n",
      "|  6|spark hadoop spark|\n",
      "|  7|     apache hadoop|\n",
      "+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions on test documents and print columns of interest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "| id|              text|               words|            features|       rawPrediction|         probability|prediction|\n",
      "+---+------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|  4|       spark i j k|    [spark, i, j, k]|(262144,[19036,68...|[-1.6609033227473...|[0.15964077387874...|       1.0|\n",
      "|  5|             l m n|           [l, m, n]|(262144,[1303,526...|[1.64218895265635...|[0.83783256854766...|       0.0|\n",
      "|  6|spark hadoop spark|[spark, hadoop, s...|(262144,[173558,1...|[-2.5980142174392...|[0.06926633132976...|       1.0|\n",
      "|  7|     apache hadoop|    [apache, hadoop]|(262144,[68303,19...|[4.00817033336806...|[0.98215753334442...|       0.0|\n",
      "+---+------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = model.transform(test)\n",
    "prediction.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+--------------------+----------+\n",
      "| id|              text|         probability|prediction|\n",
      "+---+------------------+--------------------+----------+\n",
      "|  4|       spark i j k|[0.15964077387874...|       1.0|\n",
      "|  5|             l m n|[0.83783256854766...|       0.0|\n",
      "|  6|spark hadoop spark|[0.06926633132976...|       1.0|\n",
      "|  7|     apache hadoop|[0.98215753334442...|       0.0|\n",
      "+---+------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "selected.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, spark i j k) --> prob=[0.15964077387874118,0.8403592261212589], prediction=1.000000\n",
      "(5, l m n) --> prob=[0.8378325685476612,0.16216743145233875], prediction=0.000000\n",
      "(6, spark hadoop spark) --> prob=[0.06926633132976273,0.9307336686702373], prediction=1.000000\n",
      "(7, apache hadoop) --> prob=[0.9821575333444208,0.01784246665557917], prediction=0.000000\n"
     ]
    }
   ],
   "source": [
    "for row in selected.collect():\n",
    "    rid, text, prob, prediction = row # 取变量名\n",
    "    print(\"(%d, %s) --> prob=%s, prediction=%f\" % (rid, text, str(prob), prediction))#格式化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exercise (completing three or more questions is considered as completion of this exercise)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Jupyther notebook is primarily for interactive learning that is more suitable on small scale data. For large-scale (big) data, stand-alone programs or HPC batch jobs are encouraged. \n",
    "* Load the NASA access log Aug95 data in Session 1 and create a DataFrame with FIVE columns by **specifying** the schema \n",
    "* Perform some more challenging mining tasks in Session 1 using *as many DataFrame functions as possible*\n",
    "   * How many **unique** hosts on a particular day (e.g., 15th August)?\n",
    "   * How many **unique** hosts in total (i.e., in August 1995)?\n",
    "   * Which host is the most frequent visitor?\n",
    "   * How many different types of return codes?\n",
    "   * How many requests per day on average?\n",
    "   * How many requests per post on average?\n",
    "   * Any other question that you are interested in.\n",
    "* Explore more CSV data of your interest via Google or at [Sample CSV data](https://support.spatialkey.com/spatialkey-sample-csv-data/), including insurance, real estate, and sales transactions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
