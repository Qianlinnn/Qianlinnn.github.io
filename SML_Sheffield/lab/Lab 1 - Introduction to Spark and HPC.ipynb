{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COM6012 Scalable Machine Learning 2020 - Haiping Lu\n",
    "# Lab 1 - Introduction to (Py)Spark and (Sheffield)HPC\n",
    "\n",
    "## Objectives\n",
    "\n",
    "* Task 1: To finish in the lab session. **Critical**\n",
    "* Task 2: To finish in the lab session. **Critical**\n",
    "* Task 3: To finish in the lab session. **Essential**\n",
    "* Task 4: To finish in the lab session. **Essential**\n",
    "* Task 5: To explore by yourself before the next session. **Optional but recommended**\n",
    "\n",
    "**Suggested reading**: \n",
    "* Chapters 2 to 4 of [PySpark tutorial](https://runawayhorse001.github.io/LearningApacheSpark/pyspark.pdf) (several sections in Chapter 3 can be safely skipped)\n",
    "* [Spark Quick Start](https://spark.apache.org/docs/2.3.2/quick-start.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All slides and notebooks are at: https://github.com/haipinglu/ScalableML/\n",
    "\n",
    "**<span style=\"color:red\">Note - Please read before proceeding</span>**: \n",
    "* To be safe, we will continue to use pyspark **2.3.2** rather than the latest **2.4.x** due to JVM problem on Windows encountered previously. See this [discussion](https://stackoverflow.com/questions/53161939/pyspark-error-does-not-exist-in-the-jvm-error-when-initializing-sparkcontext)\n",
    "\n",
    "* HPC are  **shared** resources relying on considerate usage of every user. When requesting resources, if you ask for too much (e.g. 50 cores), it will take a long time to get allocated and once allocated, it will leave much less for the others. If everybody is asking for too much, the system won't work.\n",
    "* When using Jupyter Hub (see below), please **logout** when you finish using, otherwise, you are occupying the **shared** resources still! If you are not sure, check the status of your job(s) with `qstat`.\n",
    "* We have five nodes (each with 40 cores, 768GB RAM, SSD) reserved for this module. You can specify `-P rse-com6012` (e.g. after `qrshx`) to get access. However, these nodes do not always be more available, e.g. if everyone of us is using it but many regular nodes are idle.\n",
    "* Please follow **all steps (step by step without skipping)** unless you are very confident in handling problems by yourself. We have a fast pace in assessment so please finish as much tasks as possible by the end of each lab. You are strongly encouraged to try out the notebooks before the lab so that if there is any problem, we can help you solve it in the lab. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Spark\n",
    "\n",
    "### 1.1a HPC - ShARC  (help: ` hpc@sheffield.ac.uk`)\n",
    "\n",
    "#### Connect to ShARC (`host: sharc.sheffield.ac.uk`) via SSH: [MobaXterm on Windows](https://www.sheffield.ac.uk/cics/research/hpc/using/access/windowspc),  [Linux/Unix](https://www.sheffield.ac.uk/cics/research/hpc/using/access/linux), [Apple/Mac](https://www.sheffield.ac.uk/cics/research/hpc/using/access/apple). See [Connecting to a cluster using SSH](http://docs.hpc.shef.ac.uk/en/latest/hpc/connecting.html), and also [Intro_to_HPC by Mike](https://github.com/mikecroucher/Intro_to_HPC) (note that we are not using Scala though)\n",
    "\n",
    "#### Start an interactive session on a node\n",
    "`qrshx`\n",
    "#### Load java and conda\n",
    "`module load apps/java/jdk1.8.0_102/binary`\n",
    "\n",
    "`module load apps/python/conda`\n",
    "\n",
    "#### Create a virtual environment called myspark\n",
    "`conda create -n myspark python=3.6`\n",
    "\n",
    "When you are asked whether to proceed, say `y`\n",
    "\n",
    "#### Activate the environment\n",
    "`source activate myspark`\n",
    "\n",
    "#### Install spark and pyspark 2.3.2 from conda-forge\n",
    "`conda install -c conda-forge pyspark=2.3.2`\n",
    "\n",
    "\n",
    "When you are asked whether to proceed, say `y`\n",
    "\n",
    "#### Run spark\n",
    "`pyspark`\n",
    "\n",
    "#### Not familiar with terminal?: [A tutorial from Mike Croucher](https://github.com/mikecroucher/Intro_to_HPC/blob/gh-pages/terminal_tutorial.md)\n",
    "\n",
    "### 1.1b Setup Jupyter Hub to run Notebook on HPC!!!! \n",
    "Now we can run jupyter notebooks on HPC. See [Using Jupyter on Sharc](http://docs.hpc.shef.ac.uk/en/latest/hpc/jupyterhub.html#using-jupyter-on-sharc). Many thanks to Will and the RSE team for making this possible, and Vamsi for making this happen.\n",
    "~~~~\n",
    "qrshx \n",
    "module load apps/java/jdk1.8.0_102/binary \n",
    "module load apps/python/conda \n",
    "conda create -n jupyter-spark python=3.6 ipykernel jupyter_client\n",
    "source activate jupyter-spark\n",
    "conda install -c conda-forge pyspark=2.3.2\n",
    "~~~~\n",
    "When you are asked whether to proceed, say `y`\n",
    "\n",
    "#### Start a [JupyterHub server](https://jupyter-sharc.shef.ac.uk/)\n",
    "* Log in the same way as you log in Sharc. Use the default settings first. You can explore later.\n",
    "* Browse to the folder of the notebooks. \n",
    "* Open the desired notebook.\n",
    "* Change the kernel \"jupyter-spark\" (choose \"Kernel\" from the menue, then \"Change kernel\")\n",
    "* Enjoy notebook on HPC\n",
    "\n",
    "To run this notebook on HPC via Jupyter Hub, we need to run the following cell. If you are running this notebook on your local machine, skip the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] 系统找不到指定的文件。",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-fd3f3b75fbac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'/usr/bin/modulecmd'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'load'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'apps/java/jdk1.8.0_102/binary'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'PYSPARK_PYTHON'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'HOME'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/.conda/envs/jupyter-spark/bin/python'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-fd3f3b75fbac>\u001b[0m in \u001b[0;36mmodule\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'/usr/bin/modulecmd'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'load'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'apps/java/jdk1.8.0_102/binary'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[0;32m    773\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    774\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 775\u001b[1;33m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[0;32m    776\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m             \u001b[1;31m# Cleanup if the child failed starting.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1176\u001b[0m                                          \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m                                          \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcwd\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcwd\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m                                          startupinfo)\n\u001b[0m\u001b[0;32m   1179\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m                 \u001b[1;31m# Child is launched. Close the parent's copy of those pipe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] 系统找不到指定的文件。"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "def module(*args):        \n",
    "    if isinstance(args[0], list):        \n",
    "        args = args[0]        \n",
    "    else:        \n",
    "        args = list(args)        \n",
    "    (output, error) = subprocess.Popen(['/usr/bin/modulecmd', 'python'] + args, stdout=subprocess.PIPE).communicate()\n",
    "    exec(output)    \n",
    "module('load', 'apps/java/jdk1.8.0_102/binary')    \n",
    "os.environ['PYSPARK_PYTHON'] = os.environ['HOME'] + '/.conda/envs/jupyter-spark/bin/python'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Windows/Linux/Mac: on your own / lab machine.  \n",
    "\n",
    "You need to follow instructions for your OS (Windows/Linux/Mac) below to install Java, set up the proper paths, etc., except that if you have `conda` installed already (e.g., from COM6509 the MLAI module), `pyspark 2.3.2` can be installed via (see above)\n",
    "\n",
    "`conda install -c conda-forge pyspark=2.3.2`\n",
    "\n",
    "\n",
    "* Windows: 1) With video - [Install Spark on Windows (PySpark)](https://medium.com/@GalarnykMichael/install-spark-on-windows-pyspark-4498a5d8d66c) 2) [How to install Spark on Windows in 5 steps](https://medium.com/@dvainrub/how-to-install-apache-spark-2-x-in-your-pc-e2047246ffc3) **Note:** The following may be needed. Go to your System Environment Variables and add PYTHONPATH to it with the following value: `%SPARK_HOME%\\python;%SPARK_HOME%\\python\\lib\\py4j-<version>-src.zip:%PYTHONPATH%`, just check what py4j version you have in your `spark/python/lib` folder ([source](https://stackoverflow.com/questions/53161939/pyspark-error-does-not-exist-in-the-jvm-error-when-initializing-sparkcontext?noredirect=1&lq=1)).\n",
    "\n",
    "* Linux: With video - [Install PySpark on Ubuntu](https://medium.com/@GalarnykMichael/install-spark-on-ubuntu-pyspark-231c45677de0)\n",
    "\n",
    "* Mac: [Install Spark/PySpark on Mac](https://medium.com/@yajieli/installing-spark-pyspark-on-mac-and-fix-of-some-common-errors-355a9050f735) **Note: you need to use Java 8. Java 11 is having problems.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\software_program\\anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - pyspark=2.3.2\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    certifi-2019.9.11          |           py37_0         147 KB  conda-forge\n",
      "    conda-4.8.2                |           py37_0         3.0 MB  conda-forge\n",
      "    py4j-0.10.7                |             py_1         177 KB  conda-forge\n",
      "    pyspark-2.3.2              |        py37_1000       202.2 MB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:       205.6 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  py4j               conda-forge/noarch::py4j-0.10.7-py_1\n",
      "  pyspark            conda-forge/win-64::pyspark-2.3.2-py37_1000\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  conda                      pkgs/main::conda-4.7.12-py37_0 --> conda-forge::conda-4.8.2-py37_0\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi                                         pkgs/main --> conda-forge\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "pyspark-2.3.2        | 202.2 MB  |            |   0% \n",
      "pyspark-2.3.2        | 202.2 MB  |            |   0% \n",
      "pyspark-2.3.2        | 202.2 MB  |            |   0% \n",
      "pyspark-2.3.2        | 202.2 MB  |            |   0% \n",
      "pyspark-2.3.2        | 202.2 MB  |            |   0% \n",
      "pyspark-2.3.2        | 202.2 MB  |            |   0% \n",
      "pyspark-2.3.2        | 202.2 MB  |            |   1% \n",
      "pyspark-2.3.2        | 202.2 MB  |            |   1% \n",
      "pyspark-2.3.2        | 202.2 MB  |            |   1% \n",
      "pyspark-2.3.2        | 202.2 MB  |            |   1% \n",
      "pyspark-2.3.2        | 202.2 MB  |            |   1% \n",
      "pyspark-2.3.2        | 202.2 MB  | 1          |   1% \n",
      "pyspark-2.3.2        | 202.2 MB  | 1          |   1% \n",
      "pyspark-2.3.2        | 202.2 MB  | 1          |   1% \n",
      "pyspark-2.3.2        | 202.2 MB  | 1          |   2% \n",
      "pyspark-2.3.2        | 202.2 MB  | 1          |   2% \n",
      "pyspark-2.3.2        | 202.2 MB  | 1          |   2% \n",
      "pyspark-2.3.2        | 202.2 MB  | 1          |   2% \n",
      "pyspark-2.3.2        | 202.2 MB  | 2          |   2% \n",
      "pyspark-2.3.2        | 202.2 MB  | 2          |   2% \n",
      "pyspark-2.3.2        | 202.2 MB  | 2          |   2% \n",
      "pyspark-2.3.2        | 202.2 MB  | 2          |   2% \n",
      "pyspark-2.3.2        | 202.2 MB  | 2          |   2% \n",
      "pyspark-2.3.2        | 202.2 MB  | 2          |   3% \n",
      "pyspark-2.3.2        | 202.2 MB  | 2          |   3% \n",
      "pyspark-2.3.2        | 202.2 MB  | 2          |   3% \n",
      "pyspark-2.3.2        | 202.2 MB  | 2          |   3% \n",
      "pyspark-2.3.2        | 202.2 MB  | 3          |   3% \n",
      "pyspark-2.3.2        | 202.2 MB  | 3          |   3% \n",
      "pyspark-2.3.2        | 202.2 MB  | 3          |   3% \n",
      "pyspark-2.3.2        | 202.2 MB  | 3          |   4% \n",
      "pyspark-2.3.2        | 202.2 MB  | 3          |   4% \n",
      "pyspark-2.3.2        | 202.2 MB  | 3          |   4% \n",
      "pyspark-2.3.2        | 202.2 MB  | 3          |   4% \n",
      "pyspark-2.3.2        | 202.2 MB  | 4          |   4% \n",
      "pyspark-2.3.2        | 202.2 MB  | 4          |   4% \n",
      "pyspark-2.3.2        | 202.2 MB  | 4          |   4% \n",
      "pyspark-2.3.2        | 202.2 MB  | 4          |   4% \n",
      "pyspark-2.3.2        | 202.2 MB  | 4          |   4% \n",
      "pyspark-2.3.2        | 202.2 MB  | 4          |   5% \n",
      "pyspark-2.3.2        | 202.2 MB  | 4          |   5% \n",
      "pyspark-2.3.2        | 202.2 MB  | 4          |   5% \n",
      "pyspark-2.3.2        | 202.2 MB  | 4          |   5% \n",
      "pyspark-2.3.2        | 202.2 MB  | 4          |   5% \n",
      "pyspark-2.3.2        | 202.2 MB  | 4          |   5% \n",
      "pyspark-2.3.2        | 202.2 MB  | 5          |   5% \n",
      "pyspark-2.3.2        | 202.2 MB  | 5          |   5% \n",
      "pyspark-2.3.2        | 202.2 MB  | 5          |   5% \n",
      "pyspark-2.3.2        | 202.2 MB  | 5          |   6% \n",
      "pyspark-2.3.2        | 202.2 MB  | 5          |   6% \n",
      "pyspark-2.3.2        | 202.2 MB  | 5          |   6% \n",
      "pyspark-2.3.2        | 202.2 MB  | 5          |   6% \n",
      "pyspark-2.3.2        | 202.2 MB  | 6          |   6% \n",
      "pyspark-2.3.2        | 202.2 MB  | 6          |   6% \n",
      "pyspark-2.3.2        | 202.2 MB  | 6          |   6% \n",
      "pyspark-2.3.2        | 202.2 MB  | 6          |   7% \n",
      "pyspark-2.3.2        | 202.2 MB  | 6          |   7% \n",
      "pyspark-2.3.2        | 202.2 MB  | 6          |   7% \n",
      "pyspark-2.3.2        | 202.2 MB  | 7          |   7% \n",
      "pyspark-2.3.2        | 202.2 MB  | 7          |   7% \n",
      "pyspark-2.3.2        | 202.2 MB  | 7          |   7% \n",
      "pyspark-2.3.2        | 202.2 MB  | 7          |   8% \n",
      "pyspark-2.3.2        | 202.2 MB  | 7          |   8% \n",
      "pyspark-2.3.2        | 202.2 MB  | 7          |   8% \n",
      "pyspark-2.3.2        | 202.2 MB  | 8          |   8% \n",
      "pyspark-2.3.2        | 202.2 MB  | 8          |   8% \n",
      "pyspark-2.3.2        | 202.2 MB  | 8          |   9% \n",
      "pyspark-2.3.2        | 202.2 MB  | 8          |   9% \n",
      "pyspark-2.3.2        | 202.2 MB  | 8          |   9% \n",
      "pyspark-2.3.2        | 202.2 MB  | 9          |   9% \n",
      "pyspark-2.3.2        | 202.2 MB  | 9          |   9% \n",
      "pyspark-2.3.2        | 202.2 MB  | 9          |   9% \n",
      "pyspark-2.3.2        | 202.2 MB  | 9          |  10% \n",
      "pyspark-2.3.2        | 202.2 MB  | 9          |  10% \n",
      "pyspark-2.3.2        | 202.2 MB  | 9          |  10% \n",
      "pyspark-2.3.2        | 202.2 MB  | #          |  10% \n",
      "pyspark-2.3.2        | 202.2 MB  | #          |  10% \n",
      "pyspark-2.3.2        | 202.2 MB  | #          |  11% \n",
      "pyspark-2.3.2        | 202.2 MB  | #          |  11% \n",
      "pyspark-2.3.2        | 202.2 MB  | #          |  11% \n",
      "pyspark-2.3.2        | 202.2 MB  | #          |  11% \n",
      "pyspark-2.3.2        | 202.2 MB  | #1         |  11% \n",
      "pyspark-2.3.2        | 202.2 MB  | #1         |  11% \n",
      "pyspark-2.3.2        | 202.2 MB  | #1         |  11% \n",
      "pyspark-2.3.2        | 202.2 MB  | #1         |  12% \n",
      "pyspark-2.3.2        | 202.2 MB  | #1         |  12% \n",
      "pyspark-2.3.2        | 202.2 MB  | #1         |  12% \n",
      "pyspark-2.3.2        | 202.2 MB  | #2         |  12% \n",
      "pyspark-2.3.2        | 202.2 MB  | #2         |  12% \n",
      "pyspark-2.3.2        | 202.2 MB  | #2         |  12% \n",
      "pyspark-2.3.2        | 202.2 MB  | #2         |  13% \n",
      "pyspark-2.3.2        | 202.2 MB  | #2         |  13% \n",
      "pyspark-2.3.2        | 202.2 MB  | #2         |  13% \n",
      "pyspark-2.3.2        | 202.2 MB  | #3         |  13% \n",
      "pyspark-2.3.2        | 202.2 MB  | #3         |  13% \n",
      "pyspark-2.3.2        | 202.2 MB  | #3         |  13% \n",
      "pyspark-2.3.2        | 202.2 MB  | #3         |  14% \n",
      "pyspark-2.3.2        | 202.2 MB  | #3         |  14% \n",
      "pyspark-2.3.2        | 202.2 MB  | #4         |  14% \n",
      "pyspark-2.3.2        | 202.2 MB  | #4         |  14% \n",
      "pyspark-2.3.2        | 202.2 MB  | #4         |  14% \n",
      "pyspark-2.3.2        | 202.2 MB  | #4         |  15% \n",
      "pyspark-2.3.2        | 202.2 MB  | #4         |  15% \n",
      "pyspark-2.3.2        | 202.2 MB  | #4         |  15% \n",
      "pyspark-2.3.2        | 202.2 MB  | #5         |  15% \n",
      "pyspark-2.3.2        | 202.2 MB  | #5         |  15% \n",
      "pyspark-2.3.2        | 202.2 MB  | #5         |  16% \n",
      "pyspark-2.3.2        | 202.2 MB  | #5         |  16% \n",
      "pyspark-2.3.2        | 202.2 MB  | #5         |  16% \n",
      "pyspark-2.3.2        | 202.2 MB  | #6         |  16% \n",
      "pyspark-2.3.2        | 202.2 MB  | #6         |  16% \n",
      "pyspark-2.3.2        | 202.2 MB  | #6         |  17% \n",
      "pyspark-2.3.2        | 202.2 MB  | #6         |  17% \n",
      "pyspark-2.3.2        | 202.2 MB  | #6         |  17% \n",
      "pyspark-2.3.2        | 202.2 MB  | #7         |  17% \n",
      "pyspark-2.3.2        | 202.2 MB  | #7         |  17% \n",
      "pyspark-2.3.2        | 202.2 MB  | #7         |  17% \n",
      "pyspark-2.3.2        | 202.2 MB  | #7         |  18% \n",
      "pyspark-2.3.2        | 202.2 MB  | #7         |  18% \n",
      "pyspark-2.3.2        | 202.2 MB  | #8         |  18% \n",
      "pyspark-2.3.2        | 202.2 MB  | #8         |  18% \n",
      "pyspark-2.3.2        | 202.2 MB  | #8         |  19% \n",
      "pyspark-2.3.2        | 202.2 MB  | #8         |  19% \n",
      "pyspark-2.3.2        | 202.2 MB  | #8         |  19% \n",
      "pyspark-2.3.2        | 202.2 MB  | #8         |  19% \n",
      "pyspark-2.3.2        | 202.2 MB  | #9         |  19% \n",
      "pyspark-2.3.2        | 202.2 MB  | #9         |  19% \n",
      "pyspark-2.3.2        | 202.2 MB  | #9         |  20% \n",
      "pyspark-2.3.2        | 202.2 MB  | #9         |  20% \n",
      "pyspark-2.3.2        | 202.2 MB  | ##         |  20% \n",
      "pyspark-2.3.2        | 202.2 MB  | ##         |  20% \n",
      "pyspark-2.3.2        | 202.2 MB  | ##         |  20% \n",
      "pyspark-2.3.2        | 202.2 MB  | ##         |  21% \n",
      "pyspark-2.3.2        | 202.2 MB  | ##         |  21% \n",
      "pyspark-2.3.2        | 202.2 MB  | ##1        |  21% \n",
      "pyspark-2.3.2        | 202.2 MB  | ##1        |  21% \n",
      "pyspark-2.3.2        | 202.2 MB  | ##1        |  22% \n",
      "pyspark-2.3.2        | 202.2 MB  | ##1        |  22% \n",
      "pyspark-2.3.2        | 202.2 MB  | ##2        |  22% \n",
      "pyspark-2.3.2        | 202.2 MB  | ##2        |  22% \n",
      "pyspark-2.3.2        | 202.2 MB  | ##2        |  23% \n",
      "pyspark-2.3.2        | 202.2 MB  | ##2        |  23% \n",
      "pyspark-2.3.2        | 202.2 MB  | ##3        |  23% \n",
      "pyspark-2.3.2        | 202.2 MB  | ##3        |  24% \n",
      "pyspark-2.3.2        | 202.2 MB  | ##3        |  24% \n",
      "pyspark-2.3.2        | 202.2 MB  | ##4        |  24% \n",
      "pyspark-2.3.2        | 202.2 MB  | ##4        |  24% \n",
      "pyspark-2.3.2        | 202.2 MB  | ##4        |  25% \n",
      "pyspark-2.3.2        | 202.2 MB  | ##4        |  25% \n",
      "pyspark-2.3.2        | 202.2 MB  | ##5        |  25% \n",
      "pyspark-2.3.2        | 202.2 MB  | ##5        |  25% \n",
      "pyspark-2.3.2        | 202.2 MB  | ##5        |  26% \n",
      "pyspark-2.3.2        | 202.2 MB  | ##5        |  26% \n",
      "pyspark-2.3.2        | 202.2 MB  | ##6        |  26% \n",
      "pyspark-2.3.2        | 202.2 MB  | ##6        |  26% \n",
      "pyspark-2.3.2        | 202.2 MB  | ##6        |  27% \n",
      "pyspark-2.3.2        | 202.2 MB  | ##6        |  27% \n",
      "pyspark-2.3.2        | 202.2 MB  | ##7        |  27% \n",
      "pyspark-2.3.2        | 202.2 MB  | ##7        |  28% \n",
      "pyspark-2.3.2        | 202.2 MB  | ##7        |  28% \n",
      "pyspark-2.3.2        | 202.2 MB  | ##8        |  28% \n",
      "pyspark-2.3.2        | 202.2 MB  | ##8        |  28% \n",
      "pyspark-2.3.2        | 202.2 MB  | ##8        |  29% \n",
      "pyspark-2.3.2        | 202.2 MB  | ##8        |  29% \n",
      "pyspark-2.3.2        | 202.2 MB  | ##9        |  29% \n",
      "pyspark-2.3.2        | 202.2 MB  | ##9        |  30% \n",
      "pyspark-2.3.2        | 202.2 MB  | ##9        |  30% \n",
      "pyspark-2.3.2        | 202.2 MB  | ###        |  30% \n",
      "pyspark-2.3.2        | 202.2 MB  | ###        |  30% \n",
      "pyspark-2.3.2        | 202.2 MB  | ###        |  31% \n",
      "pyspark-2.3.2        | 202.2 MB  | ###        |  31% \n",
      "pyspark-2.3.2        | 202.2 MB  | ###1       |  31% \n",
      "pyspark-2.3.2        | 202.2 MB  | ###1       |  31% \n",
      "pyspark-2.3.2        | 202.2 MB  | ###1       |  32% \n",
      "pyspark-2.3.2        | 202.2 MB  | ###1       |  32% \n",
      "pyspark-2.3.2        | 202.2 MB  | ###2       |  32% \n",
      "pyspark-2.3.2        | 202.2 MB  | ###2       |  32% \n",
      "pyspark-2.3.2        | 202.2 MB  | ###2       |  33% \n",
      "pyspark-2.3.2        | 202.2 MB  | ###2       |  33% \n",
      "pyspark-2.3.2        | 202.2 MB  | ###3       |  33% \n",
      "pyspark-2.3.2        | 202.2 MB  | ###3       |  33% \n",
      "pyspark-2.3.2        | 202.2 MB  | ###3       |  34% \n",
      "pyspark-2.3.2        | 202.2 MB  | ###3       |  34% \n",
      "pyspark-2.3.2        | 202.2 MB  | ###4       |  34% \n",
      "pyspark-2.3.2        | 202.2 MB  | ###4       |  34% \n",
      "pyspark-2.3.2        | 202.2 MB  | ###4       |  35% \n",
      "pyspark-2.3.2        | 202.2 MB  | ###4       |  35% \n",
      "pyspark-2.3.2        | 202.2 MB  | ###5       |  35% \n",
      "pyspark-2.3.2        | 202.2 MB  | ###5       |  35% \n",
      "pyspark-2.3.2        | 202.2 MB  | ###5       |  36% \n",
      "pyspark-2.3.2        | 202.2 MB  | ###5       |  36% \n",
      "pyspark-2.3.2        | 202.2 MB  | ###6       |  36% \n",
      "pyspark-2.3.2        | 202.2 MB  | ###6       |  37% \n",
      "pyspark-2.3.2        | 202.2 MB  | ###6       |  37% \n",
      "pyspark-2.3.2        | 202.2 MB  | ###7       |  37% \n",
      "pyspark-2.3.2        | 202.2 MB  | ###7       |  37% \n",
      "pyspark-2.3.2        | 202.2 MB  | ###7       |  38% \n",
      "pyspark-2.3.2        | 202.2 MB  | ###7       |  38% \n",
      "pyspark-2.3.2        | 202.2 MB  | ###8       |  38% \n",
      "pyspark-2.3.2        | 202.2 MB  | ###8       |  38% \n",
      "pyspark-2.3.2        | 202.2 MB  | ###8       |  39% \n",
      "pyspark-2.3.2        | 202.2 MB  | ###9       |  39% \n",
      "pyspark-2.3.2        | 202.2 MB  | ###9       |  39% \n",
      "pyspark-2.3.2        | 202.2 MB  | ###9       |  40% \n",
      "pyspark-2.3.2        | 202.2 MB  | ###9       |  40% \n",
      "pyspark-2.3.2        | 202.2 MB  | ####       |  40% \n",
      "pyspark-2.3.2        | 202.2 MB  | ####       |  40% \n",
      "pyspark-2.3.2        | 202.2 MB  | ####       |  41% \n",
      "pyspark-2.3.2        | 202.2 MB  | ####       |  41% \n",
      "pyspark-2.3.2        | 202.2 MB  | ####1      |  41% \n",
      "pyspark-2.3.2        | 202.2 MB  | ####1      |  42% \n",
      "pyspark-2.3.2        | 202.2 MB  | ####1      |  42% \n",
      "pyspark-2.3.2        | 202.2 MB  | ####2      |  42% \n",
      "pyspark-2.3.2        | 202.2 MB  | ####2      |  42% \n",
      "pyspark-2.3.2        | 202.2 MB  | ####2      |  43% \n",
      "pyspark-2.3.2        | 202.2 MB  | ####2      |  43% \n",
      "pyspark-2.3.2        | 202.2 MB  | ####3      |  43% \n",
      "pyspark-2.3.2        | 202.2 MB  | ####3      |  43% \n",
      "pyspark-2.3.2        | 202.2 MB  | ####3      |  44% \n",
      "pyspark-2.3.2        | 202.2 MB  | ####3      |  44% \n",
      "pyspark-2.3.2        | 202.2 MB  | ####4      |  44% \n",
      "pyspark-2.3.2        | 202.2 MB  | ####4      |  44% \n",
      "pyspark-2.3.2        | 202.2 MB  | ####4      |  45% \n",
      "pyspark-2.3.2        | 202.2 MB  | ####4      |  45% \n",
      "pyspark-2.3.2        | 202.2 MB  | ####5      |  45% \n",
      "pyspark-2.3.2        | 202.2 MB  | ####5      |  45% \n",
      "pyspark-2.3.2        | 202.2 MB  | ####5      |  46% \n",
      "pyspark-2.3.2        | 202.2 MB  | ####5      |  46% \n",
      "pyspark-2.3.2        | 202.2 MB  | ####6      |  46% \n",
      "pyspark-2.3.2        | 202.2 MB  | ####6      |  46% \n",
      "pyspark-2.3.2        | 202.2 MB  | ####6      |  47% \n",
      "pyspark-2.3.2        | 202.2 MB  | ####7      |  47% \n",
      "pyspark-2.3.2        | 202.2 MB  | ####7      |  47% \n",
      "pyspark-2.3.2        | 202.2 MB  | ####7      |  48% \n",
      "pyspark-2.3.2        | 202.2 MB  | ####7      |  48% \n",
      "pyspark-2.3.2        | 202.2 MB  | ####8      |  48% \n",
      "pyspark-2.3.2        | 202.2 MB  | ####8      |  48% \n",
      "pyspark-2.3.2        | 202.2 MB  | ####8      |  49% \n",
      "pyspark-2.3.2        | 202.2 MB  | ####8      |  49% \n",
      "pyspark-2.3.2        | 202.2 MB  | ####9      |  49% \n",
      "pyspark-2.3.2        | 202.2 MB  | ####9      |  50% \n",
      "pyspark-2.3.2        | 202.2 MB  | ####9      |  50% \n",
      "pyspark-2.3.2        | 202.2 MB  | #####      |  50% \n",
      "pyspark-2.3.2        | 202.2 MB  | #####      |  50% \n",
      "pyspark-2.3.2        | 202.2 MB  | #####      |  51% \n",
      "pyspark-2.3.2        | 202.2 MB  | #####      |  51% \n",
      "pyspark-2.3.2        | 202.2 MB  | #####1     |  51% \n",
      "pyspark-2.3.2        | 202.2 MB  | #####1     |  51% \n",
      "pyspark-2.3.2        | 202.2 MB  | #####1     |  52% \n",
      "pyspark-2.3.2        | 202.2 MB  | #####1     |  52% \n",
      "pyspark-2.3.2        | 202.2 MB  | #####2     |  52% \n",
      "pyspark-2.3.2        | 202.2 MB  | #####2     |  53% \n",
      "pyspark-2.3.2        | 202.2 MB  | #####2     |  53% \n",
      "pyspark-2.3.2        | 202.2 MB  | #####3     |  53% \n",
      "pyspark-2.3.2        | 202.2 MB  | #####3     |  53% \n",
      "pyspark-2.3.2        | 202.2 MB  | #####3     |  54% \n",
      "pyspark-2.3.2        | 202.2 MB  | #####3     |  54% \n",
      "pyspark-2.3.2        | 202.2 MB  | #####4     |  54% \n",
      "pyspark-2.3.2        | 202.2 MB  | #####4     |  54% \n",
      "pyspark-2.3.2        | 202.2 MB  | #####4     |  55% \n",
      "pyspark-2.3.2        | 202.2 MB  | #####5     |  55% \n",
      "pyspark-2.3.2        | 202.2 MB  | #####5     |  55% \n",
      "pyspark-2.3.2        | 202.2 MB  | #####5     |  55% \n",
      "pyspark-2.3.2        | 202.2 MB  | #####5     |  56% \n",
      "pyspark-2.3.2        | 202.2 MB  | #####6     |  56% \n",
      "pyspark-2.3.2        | 202.2 MB  | #####6     |  56% \n",
      "pyspark-2.3.2        | 202.2 MB  | #####6     |  57% \n",
      "pyspark-2.3.2        | 202.2 MB  | #####6     |  57% \n",
      "pyspark-2.3.2        | 202.2 MB  | #####7     |  57% \n",
      "pyspark-2.3.2        | 202.2 MB  | #####7     |  57% \n",
      "pyspark-2.3.2        | 202.2 MB  | #####7     |  58% \n",
      "pyspark-2.3.2        | 202.2 MB  | #####7     |  58% \n",
      "pyspark-2.3.2        | 202.2 MB  | #####8     |  58% \n",
      "pyspark-2.3.2        | 202.2 MB  | #####8     |  58% \n",
      "pyspark-2.3.2        | 202.2 MB  | #####8     |  59% \n",
      "pyspark-2.3.2        | 202.2 MB  | #####8     |  59% \n",
      "pyspark-2.3.2        | 202.2 MB  | #####9     |  59% \n",
      "pyspark-2.3.2        | 202.2 MB  | #####9     |  59% \n",
      "pyspark-2.3.2        | 202.2 MB  | #####9     |  60% \n",
      "pyspark-2.3.2        | 202.2 MB  | #####9     |  60% \n",
      "pyspark-2.3.2        | 202.2 MB  | ######     |  60% \n",
      "pyspark-2.3.2        | 202.2 MB  | ######     |  60% \n",
      "pyspark-2.3.2        | 202.2 MB  | ######     |  61% \n",
      "pyspark-2.3.2        | 202.2 MB  | ######1    |  61% \n",
      "pyspark-2.3.2        | 202.2 MB  | ######1    |  61% \n",
      "pyspark-2.3.2        | 202.2 MB  | ######1    |  62% \n",
      "pyspark-2.3.2        | 202.2 MB  | ######1    |  62% \n",
      "pyspark-2.3.2        | 202.2 MB  | ######2    |  62% \n",
      "pyspark-2.3.2        | 202.2 MB  | ######2    |  62% \n",
      "pyspark-2.3.2        | 202.2 MB  | ######2    |  63% \n",
      "pyspark-2.3.2        | 202.2 MB  | ######3    |  63% \n",
      "pyspark-2.3.2        | 202.2 MB  | ######3    |  63% \n",
      "pyspark-2.3.2        | 202.2 MB  | ######3    |  64% \n",
      "pyspark-2.3.2        | 202.2 MB  | ######3    |  64% \n",
      "pyspark-2.3.2        | 202.2 MB  | ######4    |  64% \n",
      "pyspark-2.3.2        | 202.2 MB  | ######4    |  64% \n",
      "pyspark-2.3.2        | 202.2 MB  | ######4    |  65% \n",
      "pyspark-2.3.2        | 202.2 MB  | ######4    |  65% \n",
      "pyspark-2.3.2        | 202.2 MB  | ######5    |  65% \n",
      "pyspark-2.3.2        | 202.2 MB  | ######5    |  65% \n",
      "pyspark-2.3.2        | 202.2 MB  | ######5    |  66% \n",
      "pyspark-2.3.2        | 202.2 MB  | ######6    |  66% \n",
      "pyspark-2.3.2        | 202.2 MB  | ######6    |  66% \n",
      "pyspark-2.3.2        | 202.2 MB  | ######6    |  67% \n",
      "pyspark-2.3.2        | 202.2 MB  | ######6    |  67% \n",
      "pyspark-2.3.2        | 202.2 MB  | ######7    |  67% \n",
      "pyspark-2.3.2        | 202.2 MB  | ######7    |  67% \n",
      "pyspark-2.3.2        | 202.2 MB  | ######7    |  68% \n",
      "pyspark-2.3.2        | 202.2 MB  | ######7    |  68% \n",
      "pyspark-2.3.2        | 202.2 MB  | ######8    |  68% \n",
      "pyspark-2.3.2        | 202.2 MB  | ######8    |  68% \n",
      "pyspark-2.3.2        | 202.2 MB  | ######8    |  69% \n",
      "pyspark-2.3.2        | 202.2 MB  | ######8    |  69% \n",
      "pyspark-2.3.2        | 202.2 MB  | ######9    |  69% \n",
      "pyspark-2.3.2        | 202.2 MB  | ######9    |  69% \n",
      "pyspark-2.3.2        | 202.2 MB  | ######9    |  70% \n",
      "pyspark-2.3.2        | 202.2 MB  | ######9    |  70% \n",
      "pyspark-2.3.2        | 202.2 MB  | #######    |  70% \n",
      "pyspark-2.3.2        | 202.2 MB  | #######    |  70% \n",
      "pyspark-2.3.2        | 202.2 MB  | #######    |  71% \n",
      "pyspark-2.3.2        | 202.2 MB  | #######    |  71% \n",
      "pyspark-2.3.2        | 202.2 MB  | #######1   |  71% \n",
      "pyspark-2.3.2        | 202.2 MB  | #######1   |  71% \n",
      "pyspark-2.3.2        | 202.2 MB  | #######1   |  72% \n",
      "pyspark-2.3.2        | 202.2 MB  | #######2   |  72% \n",
      "pyspark-2.3.2        | 202.2 MB  | #######2   |  72% \n",
      "pyspark-2.3.2        | 202.2 MB  | #######2   |  73% \n",
      "pyspark-2.3.2        | 202.2 MB  | #######2   |  73% \n",
      "pyspark-2.3.2        | 202.2 MB  | #######3   |  73% \n",
      "pyspark-2.3.2        | 202.2 MB  | #######3   |  73% \n",
      "pyspark-2.3.2        | 202.2 MB  | #######3   |  74% \n",
      "pyspark-2.3.2        | 202.2 MB  | #######3   |  74% \n",
      "pyspark-2.3.2        | 202.2 MB  | #######4   |  74% \n",
      "pyspark-2.3.2        | 202.2 MB  | #######4   |  74% \n",
      "pyspark-2.3.2        | 202.2 MB  | #######4   |  75% \n",
      "pyspark-2.3.2        | 202.2 MB  | #######4   |  75% \n",
      "pyspark-2.3.2        | 202.2 MB  | #######5   |  75% \n",
      "pyspark-2.3.2        | 202.2 MB  | #######5   |  75% \n",
      "pyspark-2.3.2        | 202.2 MB  | #######5   |  76% \n",
      "pyspark-2.3.2        | 202.2 MB  | #######6   |  76% \n",
      "pyspark-2.3.2        | 202.2 MB  | #######6   |  76% \n",
      "pyspark-2.3.2        | 202.2 MB  | #######6   |  77% \n",
      "pyspark-2.3.2        | 202.2 MB  | #######6   |  77% \n",
      "pyspark-2.3.2        | 202.2 MB  | #######7   |  77% \n",
      "pyspark-2.3.2        | 202.2 MB  | #######7   |  77% \n",
      "pyspark-2.3.2        | 202.2 MB  | #######7   |  78% \n",
      "pyspark-2.3.2        | 202.2 MB  | #######7   |  78% \n",
      "pyspark-2.3.2        | 202.2 MB  | #######8   |  78% \n",
      "pyspark-2.3.2        | 202.2 MB  | #######8   |  78% \n",
      "pyspark-2.3.2        | 202.2 MB  | #######8   |  79% \n",
      "pyspark-2.3.2        | 202.2 MB  | #######8   |  79% \n",
      "pyspark-2.3.2        | 202.2 MB  | #######9   |  79% \n",
      "pyspark-2.3.2        | 202.2 MB  | #######9   |  79% \n",
      "pyspark-2.3.2        | 202.2 MB  | #######9   |  80% \n",
      "pyspark-2.3.2        | 202.2 MB  | #######9   |  80% \n",
      "pyspark-2.3.2        | 202.2 MB  | ########   |  80% \n",
      "pyspark-2.3.2        | 202.2 MB  | ########   |  80% \n",
      "pyspark-2.3.2        | 202.2 MB  | ########   |  81% \n",
      "pyspark-2.3.2        | 202.2 MB  | ########   |  81% \n",
      "pyspark-2.3.2        | 202.2 MB  | ########1  |  81% \n",
      "pyspark-2.3.2        | 202.2 MB  | ########1  |  81% \n",
      "pyspark-2.3.2        | 202.2 MB  | ########1  |  82% \n",
      "pyspark-2.3.2        | 202.2 MB  | ########1  |  82% \n",
      "pyspark-2.3.2        | 202.2 MB  | ########2  |  82% \n",
      "pyspark-2.3.2        | 202.2 MB  | ########2  |  82% \n",
      "pyspark-2.3.2        | 202.2 MB  | ########2  |  83% \n",
      "pyspark-2.3.2        | 202.2 MB  | ########2  |  83% \n",
      "pyspark-2.3.2        | 202.2 MB  | ########3  |  83% \n",
      "pyspark-2.3.2        | 202.2 MB  | ########3  |  84% \n",
      "pyspark-2.3.2        | 202.2 MB  | ########3  |  84% \n",
      "pyspark-2.3.2        | 202.2 MB  | ########4  |  84% \n",
      "pyspark-2.3.2        | 202.2 MB  | ########4  |  84% \n",
      "pyspark-2.3.2        | 202.2 MB  | ########4  |  85% \n",
      "pyspark-2.3.2        | 202.2 MB  | ########4  |  85% \n",
      "pyspark-2.3.2        | 202.2 MB  | ########5  |  85% \n",
      "pyspark-2.3.2        | 202.2 MB  | ########5  |  85% \n",
      "pyspark-2.3.2        | 202.2 MB  | ########5  |  86% \n",
      "pyspark-2.3.2        | 202.2 MB  | ########5  |  86% \n",
      "pyspark-2.3.2        | 202.2 MB  | ########6  |  86% \n",
      "pyspark-2.3.2        | 202.2 MB  | ########6  |  87% \n",
      "pyspark-2.3.2        | 202.2 MB  | ########6  |  87% \n",
      "pyspark-2.3.2        | 202.2 MB  | ########7  |  87% \n",
      "pyspark-2.3.2        | 202.2 MB  | ########7  |  87% \n",
      "pyspark-2.3.2        | 202.2 MB  | ########7  |  88% \n",
      "pyspark-2.3.2        | 202.2 MB  | ########7  |  88% \n",
      "pyspark-2.3.2        | 202.2 MB  | ########8  |  88% \n",
      "pyspark-2.3.2        | 202.2 MB  | ########8  |  88% \n",
      "pyspark-2.3.2        | 202.2 MB  | ########8  |  89% \n",
      "pyspark-2.3.2        | 202.2 MB  | ########8  |  89% \n",
      "pyspark-2.3.2        | 202.2 MB  | ########9  |  89% \n",
      "pyspark-2.3.2        | 202.2 MB  | ########9  |  89% \n",
      "pyspark-2.3.2        | 202.2 MB  | ########9  |  90% \n",
      "pyspark-2.3.2        | 202.2 MB  | ########9  |  90% \n",
      "pyspark-2.3.2        | 202.2 MB  | #########  |  90% \n",
      "pyspark-2.3.2        | 202.2 MB  | #########  |  90% \n",
      "pyspark-2.3.2        | 202.2 MB  | #########  |  91% \n",
      "pyspark-2.3.2        | 202.2 MB  | #########  |  91% \n",
      "pyspark-2.3.2        | 202.2 MB  | #########1 |  91% \n",
      "pyspark-2.3.2        | 202.2 MB  | #########1 |  91% \n",
      "pyspark-2.3.2        | 202.2 MB  | #########1 |  92% \n",
      "pyspark-2.3.2        | 202.2 MB  | #########1 |  92% \n",
      "pyspark-2.3.2        | 202.2 MB  | #########2 |  92% \n",
      "pyspark-2.3.2        | 202.2 MB  | #########2 |  92% \n",
      "pyspark-2.3.2        | 202.2 MB  | #########2 |  93% \n",
      "pyspark-2.3.2        | 202.2 MB  | #########2 |  93% \n",
      "pyspark-2.3.2        | 202.2 MB  | #########3 |  93% \n",
      "pyspark-2.3.2        | 202.2 MB  | #########3 |  93% \n",
      "pyspark-2.3.2        | 202.2 MB  | #########3 |  94% \n",
      "pyspark-2.3.2        | 202.2 MB  | #########3 |  94% \n",
      "pyspark-2.3.2        | 202.2 MB  | #########4 |  94% \n",
      "pyspark-2.3.2        | 202.2 MB  | #########4 |  94% \n",
      "pyspark-2.3.2        | 202.2 MB  | #########4 |  95% \n",
      "pyspark-2.3.2        | 202.2 MB  | #########4 |  95% \n",
      "pyspark-2.3.2        | 202.2 MB  | #########5 |  95% \n",
      "pyspark-2.3.2        | 202.2 MB  | #########5 |  96% \n",
      "pyspark-2.3.2        | 202.2 MB  | #########5 |  96% \n",
      "pyspark-2.3.2        | 202.2 MB  | #########6 |  96% \n",
      "pyspark-2.3.2        | 202.2 MB  | #########6 |  96% \n",
      "pyspark-2.3.2        | 202.2 MB  | #########6 |  97% \n",
      "pyspark-2.3.2        | 202.2 MB  | #########6 |  97% \n",
      "pyspark-2.3.2        | 202.2 MB  | #########7 |  97% \n",
      "pyspark-2.3.2        | 202.2 MB  | #########7 |  97% \n",
      "pyspark-2.3.2        | 202.2 MB  | #########7 |  98% \n",
      "pyspark-2.3.2        | 202.2 MB  | #########8 |  98% \n",
      "pyspark-2.3.2        | 202.2 MB  | #########8 |  98% \n",
      "pyspark-2.3.2        | 202.2 MB  | #########8 |  99% \n",
      "pyspark-2.3.2        | 202.2 MB  | #########8 |  99% \n",
      "pyspark-2.3.2        | 202.2 MB  | #########9 |  99% \n",
      "pyspark-2.3.2        | 202.2 MB  | #########9 |  99% \n",
      "pyspark-2.3.2        | 202.2 MB  | #########9 | 100% \n",
      "pyspark-2.3.2        | 202.2 MB  | #########9 | 100% \n",
      "pyspark-2.3.2        | 202.2 MB  | ########## | 100% \n",
      "\n",
      "certifi-2019.9.11    | 147 KB    |            |   0% \n",
      "certifi-2019.9.11    | 147 KB    | ########## | 100% \n",
      "\n",
      "py4j-0.10.7          | 177 KB    |            |   0% \n",
      "py4j-0.10.7          | 177 KB    | ########## | 100% \n",
      "\n",
      "conda-4.8.2          | 3.0 MB    |            |   0% \n",
      "conda-4.8.2          | 3.0 MB    | #2         |  12% \n",
      "conda-4.8.2          | 3.0 MB    | ###1       |  32% \n",
      "conda-4.8.2          | 3.0 MB    | ####8      |  49% \n",
      "conda-4.8.2          | 3.0 MB    | ######4    |  65% \n",
      "conda-4.8.2          | 3.0 MB    | ########3  |  84% \n",
      "conda-4.8.2          | 3.0 MB    | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.7.12\n",
      "  latest version: 4.8.2\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge pyspark=2.3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quiz Pyspark shell by `Ctrl+D`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run Spark\n",
    "\n",
    "### On HPC: connect and activate first\n",
    "\n",
    "`qrshx`\n",
    "\n",
    "`module load apps/java/jdk1.8.0_102/binary`\n",
    "\n",
    "`module load apps/python/conda`\n",
    "\n",
    "`source activate myspark`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive (HPC or local machine)\n",
    "\n",
    "#### If running notebook of spark on your local machine\n",
    "**Note** If `import pyspark` reports error, you may try `pip install findspark`, `import findspark`, \n",
    "`findspark.init()`, and then `import pyspark` should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[2]\").appName(\"COM6012 Spark Intro\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import findspark\n",
    "#findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[2]\").appName(\"COM6012 Spark Intro\").getOrCreate() #必须创建一个SparkSession \n",
    "                            # master是确定主参数（\"local[2]\"）是代表在本地使用2个线程来运行程序， 然后是名字\n",
    "sc = spark.sparkContext    #然后创建一个sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If running spark in a shell on either HPC or your local machine, `spark` (SparkSession) and `sc` (SparkContext) is automatically created.\n",
    "\n",
    "Run pyspark (optionally, specify to use multiple cores)\n",
    "\n",
    "`pyspark` or `pyspark --master local[2]` with two cores\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your SparkSession and SparkContext object (you will see different output if running in a shell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-E87VHT7.mynet:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>COM6012 Spark Intro</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2af36c0afc8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark # 驱动程序？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and check sc (SparkContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-E87VHT7.mynet:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>COM6012 Spark Intro</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=COM6012 Spark Intro>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc # 工人程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = sc.parallelize([1,2,3,4]) #创建RDD\n",
    "print(nums)\n",
    "nums.map(lambda x: x*x).collect() # map函数属于transformation， collect属于action函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Log Mining with Spark - Example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example deals with **Semi-Structured** data in a text file. \n",
    "\n",
    "Firstly, you need to **make sure the file is in the proper directory and change the file path if necessary**, on either HPC or local machine.\n",
    "\n",
    "**If running on HPC, you need to transfer files there.** Here is how to [**transfer files to HPC**](https://www.sheffield.ac.uk/cics/research/hpc/using/access). Please **click** and follow the instructions unless you are already familiar with it.\n",
    "\n",
    "### GUI-based file transfer\n",
    "\n",
    "* [**MobaXterm**](https://mobaxterm.mobatek.net/) is recommended for **Windows**\n",
    "* [**Cyberduck**](https://en.wikipedia.org/wiki/Cyberduck) or [**FileZilla**](https://en.wikipedia.org/wiki/FileZilla) is recommended for **Mac**\n",
    "* **FileZilla** is recommended for **Linux (e.g., Ubuntu)**\n",
    "\n",
    "For example, in MobaXterm (for Windows), you just need to **Drag your file or folder to the left directory pane of MobaXterm**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Path does not exist: file:/C:/Github/SML/ScalableML/Data/NASA_Aug95_100.txt;'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\software_program\\spark-2.4.5-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\software_program\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o56.text.\n: org.apache.spark.sql.AnalysisException: Path does not exist: file:/C:/Github/SML/ScalableML/Data/NASA_Aug95_100.txt;\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:558)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:545)\r\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\r\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\r\n\tat scala.collection.immutable.List.flatMap(List.scala:355)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:545)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:359)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\r\n\tat org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:715)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-60562e1f8c9e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlength\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mlogFile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:/Github/SML/ScalableML/Data/NASA_Aug95_100.txt\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#读取txt 文件, 得到的文件格式是Dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mlogFile\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\software_program\\spark-2.4.5-bin-hadoop2.7\\python\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mtext\u001b[1;34m(self, paths, wholetext, lineSep)\u001b[0m\n\u001b[0;32m    341\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m             \u001b[0mpaths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpaths\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 343\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\software_program\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\software_program\\spark-2.4.5-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: 'Path does not exist: file:/C:/Github/SML/ScalableML/Data/NASA_Aug95_100.txt;'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import length,mean\n",
    "\n",
    "logFile=spark.read.text(\"C:/Github/SML/ScalableML/Data/NASA_Aug95_100.txt\") #读取txt 文件, 得到的文件格式是Dataframe\n",
    "logFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logFile.count() #action函数 返回有多少行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(value='in24.inetnebr.com - - [01/Aug/1995:00:00:01 -0400] \"GET /shuttle/missions/sts-68/news/sts-68-mcc-05.txt HTTP/1.0\" 200 1839')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logFile.first() #返回该文件第一行是什么"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: How many accesses are from Japan?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|kgtyk4.kj.yamagat...|\n",
      "|kgtyk4.kj.yamagat...|\n",
      "|kgtyk4.kj.yamagat...|\n",
      "|kgtyk4.kj.yamagat...|\n",
      "|kgtyk4.kj.yamagat...|\n",
      "|kgtyk4.kj.yamagat...|\n",
      "|endeavor.fujitsu....|\n",
      "|rpgopher.aist.go....|\n",
      "|rpgopher.aist.go....|\n",
      "|rpgopher.aist.go....|\n",
      "|rpgopher.aist.go....|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hostsJapan = logFile.filter(logFile.value.contains(\".jp\"))#返回logfile里面value值包含日本的\n",
    "hostsJapan.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check whether you are getting what you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                         |\n",
      "+--------------------------------------------------------------------------------------------------------------+\n",
      "|kgtyk4.kj.yamagata-u.ac.jp - - [01/Aug/1995:00:00:17 -0400] \"GET / HTTP/1.0\" 200 7280                         |\n",
      "|kgtyk4.kj.yamagata-u.ac.jp - - [01/Aug/1995:00:00:18 -0400] \"GET /images/ksclogo-medium.gif HTTP/1.0\" 200 5866|\n",
      "|kgtyk4.kj.yamagata-u.ac.jp - - [01/Aug/1995:00:00:21 -0400] \"GET /images/NASA-logosmall.gif HTTP/1.0\" 304 0   |\n",
      "|kgtyk4.kj.yamagata-u.ac.jp - - [01/Aug/1995:00:00:21 -0400] \"GET /images/MOSAIC-logosmall.gif HTTP/1.0\" 304 0 |\n",
      "|kgtyk4.kj.yamagata-u.ac.jp - - [01/Aug/1995:00:00:22 -0400] \"GET /images/USA-logosmall.gif HTTP/1.0\" 304 0    |\n",
      "+--------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hostsJapan.show(5,False)  # 展示前五名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|length|\n",
      "+------+\n",
      "| 103.0|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hostsJapan.select(mean(length('value')).alias('length')).show() #alias 是命名 求的是所有的行数的长度均值\n",
    "            # select参数直接用字符串只能用DataFrame中的命名字段名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hostsJapan.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-contained Application\n",
    "\n",
    "To run a self-contained application, you need to **exit your shell, by `Ctrl+D` first**.\n",
    "\n",
    "Create a file `LogMining100.py`\n",
    "\n",
    "~~~~\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .appName(\"COM6012 Spark Intro\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"WARN\")  日志分为4个级别，info， error， warn debug，这里选取了WARN级别\n",
    "\n",
    "logFile=spark.read.text(\"Data/NASA_Aug95_100.txt\")\n",
    "hostsJapan = logFile.filter(logFile.value.contains(\".jp\")).count()\n",
    "\n",
    "print(\"\\n\\nHello Spark: There are %i hosts from Japan.\\n\\n\" % (hostsJapan))\n",
    "\n",
    "spark.stop()\n",
    "~~~~\n",
    "\n",
    "\n",
    "Then run it with `spark-submit Code/LogMining100.py`  **Note: You need exit your shell, by `Ctrl+C` first**\n",
    "在cmd里面运行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 4. Big Data Log Mining with Spark \n",
    "\n",
    "**Data**: Download the August data in gzip (NASA_access_log_Aug95.gz) from [NASA HTTP server access log](http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html) and put into your `Data` folder. `NASA_Aug95_100.txt` above is the first 100 lines of the August data.\n",
    "\n",
    "**Question**: How many accesses are from Japan and UK respectively?\n",
    "\n",
    "Create a file `LogMiningBig.py`\n",
    "\n",
    "~~~~\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .appName(\"COM6012 Spark Intro\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"WARN\")\n",
    "\n",
    "logFile=spark.read.text(\"../Data/NASA_access_log_Aug95.gz\").cache()\n",
    "\n",
    "hostsJapan = logFile.filter(logFile.value.contains(\".jp\")).count()\n",
    "hostsUK = logFile.filter(logFile.value.contains(\".uk\")).count()\n",
    "\n",
    "print(\"\\n\\nHello Spark: There are %i hosts from UK.\\n\" % (hostsUK))\n",
    "print(\"Hello Spark: There are %i hosts from Japan.\\n\\n\" % (hostsJapan))\n",
    "\n",
    "spark.stop()\n",
    "~~~~\n",
    "**Spark can read gzip file directly. You do not need to unzip it to a big file.**\n",
    "\n",
    "**Note the use of cache() above**\n",
    "\n",
    "### Run a program in batch mode\n",
    "\n",
    "[How to submi batch jobs to ShARC](https://www.sheffield.ac.uk/cics/research/hpc/sharc/batch) **The more resources you request, the longer you need to queue**\n",
    "\n",
    "Interactive mode will be good for learning, exploring and debugging, with smaller data. For big data, it will be more convenient to use batch processing. You submit the job to the node to join a queue. Once allocated, your job will run, with output properly recorded. This is done via a shell script.\n",
    "\n",
    "Create a file `Lab1_SubmitBatch.sh`\n",
    "\n",
    "~~~~\n",
    "#!/bin/bash\n",
    "#$ -l h_rt=2:00:00  #time needed\n",
    "#$ -pe smp 2 #number of cores\n",
    "#$ -l rmem=4G #number of memery\n",
    "#$ -o COM6012_Lab1.output #This is where your output and errors are logged.\n",
    "#$ -j y # normal and error outputs into a single file (the file above)\n",
    "#$ -M youremail@shef.ac.uk #Notify you by email, remove this line if you don't like\n",
    "#$ -m ea #Email you when it finished or aborted\n",
    "#$ -cwd # Run job from current directory\n",
    "\n",
    "module load apps/java/jdk1.8.0_102/binary\n",
    "\n",
    "module load apps/python/conda\n",
    "\n",
    "source activate myspark\n",
    "\n",
    "spark-submit ../Code/LogMiningBig.py\n",
    "~~~~\n",
    "\n",
    "* Get necessary files on your ShARC.\n",
    "* Start a session with command `qrshx`\n",
    "* Under appopriate directory (`HPC`) submit yur job via the `qsub` comand\n",
    "\n",
    "`qsub Lab1_SubmitBatch.sh`\n",
    "\n",
    "Check the status of your quening/running job(s) `qstat` (jobs not shown are finished already).\n",
    "\n",
    "Check your output file, which is **`COM6012_Lab1.output`** specified with option **`-o`** above. You can change it to a name you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[value: string]\n",
      "+------------------+\n",
      "|        mean value|\n",
      "+------------------+\n",
      "|108.84130386371228|\n",
      "+------------------+\n",
      "\n",
      "Hello Spark: There are 71600 hosts from Japan.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .appName(\"COM6012 Spark Intro\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"WARN\")\n",
    "\n",
    "logFile=spark.read.text(\"C:/Github/SML/ScalableML/Data/NASA_access_log_Aug95.gz\").cache()#\n",
    "print(logFile)\n",
    "\n",
    "hostsJapan = logFile.filter(logFile.value.contains(\".jp\")).count()\n",
    "hostsUK = logFile.filter(logFile.value.contains(\".uk\"))\n",
    "meanvalueUK = hostsUK.select(mean(length('value')). alias('mean value')).show() \n",
    "\n",
    "#print(\"\\n\\nHello Spark: There are %i hosts from UK.\\n\" % (hostsUK))\n",
    "print(\"Hello Spark: There are %i hosts from Japan.\\n\\n\" % (hostsJapan))\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exercise\n",
    "\n",
    "### More mining questions (completing three or more questions is considered as completion of this exercise):\n",
    "\n",
    "#### Easier questions (recommended)\n",
    "* How many requests in total?\n",
    "* How many requests on a particular day (e.g., 15th August)?\n",
    "* How many 404 (page not found) errors in total?\n",
    "* How many 404 (page not found) errors on a particular day (e.g., 15th August)?\n",
    "* How many requests from a particular host (e.g.,uplherc.up.com)?\n",
    "* Any other question that you are interested in.\n",
    "\n",
    "#### More challenging questions that will become easier to answer in Session 2 (optional for Session 1)\n",
    "* How many **unique** hosts on a particular day (e.g., 15th August)?\n",
    "* How many **unique** hosts in total (i.e., in August 1995)?\n",
    "* Which host is the most frequent visitor?\n",
    "* How many different types of return codes?\n",
    "* How many requests per day on average?\n",
    "* How many requests per post on average?\n",
    "* Any other question that you are interested in.\n",
    "\n",
    "### The effects of caching (recommended)\n",
    "* **Compare** the time taken to complete your jobs **with and without** `cache()`.\n",
    "\n",
    "# Acknowledgements\n",
    "\n",
    "Many thanks to Twin, Will, Mike, Vamsi for their kind help and all those kind contributors of open resources.\n",
    "\n",
    "The log mining problem is adapted from [UC Berkeley cs105x L3](https://www.edx.org/course/introduction-apache-spark-uc-berkeleyx-cs105x)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
