## 递归何时停止？ When does the recursion stop?]

该算法定义了递归停止并构造叶节点的三种情况：

* 数据集中的所有实例具有相同的分类（目标特征值），然后返回以该分类为标签的叶节点树。
* 待测试的功能集为空，然后返回一个叶子节点树，以数据集的多数类作为其分类。
* 如果数据集为空，则在进行递归调用的父节点上返回具有数据集多数类的叶节点

## 使用SLOPE拆分D8之后的决策树

![1]()

## D18该怎么操作

我们使用第三种情况来确定叶子：

1. 数据集中的所有实例具有相同的分类（目标特征值），然后返回以该分类为标签的叶节点树。
2. 待测试的功能集为空，然后返回一个叶子节点树，以数据集的多数类作为其分类。

3. 如果数据集为空，则在进行递归调用的父节点上返回具有数据集多数类的叶节点

## Regression Trees

* 构造回归树以减少树中每个叶节点上的训练样例集的方差

* 我们可以通过在选择最佳属性时使ID3算法适应变化量（a measure of variance）而不是分类杂质（熵）的方法来做到这一点。

# Impurity for regression(回归的不纯度)

* 节点处的杂质（方差）可以使用以下公式计算：

  ![2]()

* 我们选择的功能是使结果分区之间的加权方差最小

​        ![3]()

## Gini index

* 另一个常用的杂质度量是基尼系数

![4]()

* Gini索引可以被认为是计算如果您根据数据集中的分类分布对实例进行分类，则会对数据集中的实例进行误分类的频率。
* 通过使用Gini指数代替熵测度，可以使用Gini指数计算信息增益。

## Information gain ratio

* 基于熵的信息增益偏爱具有很多值的特征
* 解决此问题的一种方法是使用信息增益比，该信息增益比是通过将特征的信息增益除以用于确定特征值的信息量来计算的：

![5]()

## Why overfitting

* 随着树变得越来越深，发生过度拟合的可能性增加，这是因为在路径中的每个特征测试后，数据集被划分后，结果分类基于越来越少的子集

## Pruning strategies 修剪策略

* Pre-pruning: stop the recursive partitioning early. Pre-pruning is also  known as forward pruning.预修剪：尽早停止递归分区。 预修剪也称为正向修剪。
* 如果误差的减少不足以证明增加额外子树的额外复杂性，我们可以停止增加树的生长。
* Post-pruning后修剪：允许算法使树生长到所需程度，然后修剪导致过度拟合的分支树。

## Common Post-pruning Approach 常用的后修剪方法

* 使用验证集评估完全生长的树和修剪后的树副本所达到的预测准确性。
* 如果修剪的树副本的性能不比完全生长的树差，则该节点是修剪的候选对象。

## Model Ensembles Definition

* 与其创建单个模型，不如生成一组模型，然后通过汇总这些模型的输出进行预测。
* 由一组模型组成的预测模型称为模型集合。
* 为了使此方法起作用，集成中的模型必须彼此不同

## Approach: Two standard approaches 

1. boosting
2. bagging

# Boosting: 如何运作？

* 通过迭代创建模型并将其添加到集合中来增强工作。
* 添加预定义数量的模型后，迭代将停止。
* 当使用boosting功能时，添加到集合中的每个新模型都会偏向于更加关注先前模型未分类的实例。
* 这是通过逐步调整用于训练模型的数据集来完成的。 为此，我们使用加权数据集

## Boosting: 加权数据集

* 每个实例的相关权重wi≥0，
* 最初设置为1/n，其中n是数据集中的实例数。
* 在将每个模型添加到集合中之后，将在训练数据上对其进行测试，并减少模型正确的实例的权重，并增加模型错误的实例的权重。
* 这些权重用作分布，对数据集进行采样以创建复制的训练集，其中实例的复制与其权重成正比。

## Algorithm

在每次训练迭代中，算法：

1. 得出模型并预测总误差,通过对训练实例的模型的预测不正确的权重求和，。

   ![6]()

## Prediction

* 一旦创建了模型集，集成就使用各个模型做出的预测的加权集合进行预测。
* 此聚合中使用的权重只是与每个模型关联的置信度。

## Bagging: Definition 定义

* 当我们使用bagging (or bootstrap aggregating)时，集成中的每个模型都是在称为引导样本的数据集的随机样本上训练的。
* 每个随机样本的大小与数据集的大小相同，并使用替换样本。
* 因此，每个引导程序样本将丢失数据集中的某些实例，因此每个引导程序样本将有所不同，这意味着在不同的引导程序样本上训练的模型也将有所不同

## Bagging: 随机森林

* 当将 bagging 与决策树一起使用时，每个引导程序样本仅使用数据集中描述性特征的随机选择子集。 这称为子空间采样。
* bagging, subspace sampling, and decision trees的组合就叫随机森林模型

## boosting and bagging的实证结果

* 我们应该使用哪种方法？bagging比boost更易于实现和并行化，因此在易用性和培训时间方面可能更好。

* 实证结果表明：
  * 对于包含多达4,000个描述性特征的数据集而言，增强型决策树集成是性能最佳的模型。
  * 对于包含超过4,000个特征的数据集，随机森林合奏（基于装袋）表现更好。