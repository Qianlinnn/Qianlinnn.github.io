# Decision trees: basic concepts

## Informative features 信息特征

* 决策树是一种机器学习算法，旨在使用信息最多的特征来构建预测模型。
* 信息性特征是一种特征，其值将数据集中的实例相对于目标值划分为同类集合。

##  Does the person wear glasses?

![1]()

在两个图中，到叶子节点有4条路：
–一个路径是1个问题，
–一条路径是2个问题，
–两条路径长3个问题



平均每场游戏要问的问题数是：（1 + 2 + 3 + 3）/4 = 2.25

![2]()

 到叶子节点有4条路

平均每场游戏要问的问题数是：(2 + 2 + 2 + 2 )/ 4 = 2

平均而言，获得问题（1）的答案似乎比为问题（2）的答案提供更多的信息：后续问题更少。

## Big Idea

因此，这里的主要思想是弄清楚哪些功能是最有用的功能。



我们通过考虑以下问题的不同答案的影响来做到这一点：

*  收到答案后如何拆分域，
* 以及每个答案的可能性。

## Nodes in a decision tree

* 决策树包括：
  * 根节点
  * 内部节点 interior nodes
  * leaf nodes 叶节点
* 树中的每个非叶子节点（根节点和内部节点）都指定了要对查询的一种描述性功能进行的测试
* 每个叶节点为查询指定预测的分类

## Which tree to use?

* 对于数据集中的所有示例，这两个树都将返回相同的预测。

## Occam’s Razor

* 应用与Guess-Who游戏中相同的方法：更喜欢使用较少测试的决策树（浅树）。
* “在相互竞争的假设中，应该选择假设最少的那个”。

## How do we create shallow trees?

* 在根部测试“可疑单词”的树很浅，因为“可疑单词”功能可以将数据完美地分为“垃圾邮件”和“火腿”纯组。
* 相对于目标特征将数据集分为纯集的描述性特征可提供有关目标特征的信息
* 因此，我们可以通过在树的早期测试信息特征来制作浅树。
* 我们所需要做的就是计算集合纯度的度量：熵





