# Decision trees: basic concepts

## Informative features 信息特征

* 决策树是一种机器学习算法，旨在使用信息最多的特征来构建预测模型。
* 信息性特征是一种特征，其值将数据集中的实例相对于目标值划分为同类集合。

##  Does the person wear glasses?

![1]()

在两个图中，到叶子节点有4条路：
–一个路径是1个问题，
–一条路径是2个问题，
–两条路径长3个问题



平均每场游戏要问的问题数是：（1 + 2 + 3 + 3）/4 = 2.25

![2]()

 到叶子节点有4条路

平均每场游戏要问的问题数是：(2 + 2 + 2 + 2 )/ 4 = 2

平均而言，获得问题（1）的答案似乎比为问题（2）的答案提供更多的信息：后续问题更少。

## Big Idea

因此，这里的主要思想是弄清楚哪些功能是最有用的功能。



我们通过考虑以下问题的不同答案的影响来做到这一点：

*  收到答案后如何拆分域，
* 以及每个答案的可能性。

## Nodes in a decision tree

* 决策树包括：
  * 根节点
  * 内部节点 interior nodes
  * leaf nodes 叶节点
* 树中的每个非叶子节点（根节点和内部节点）都指定了要对查询的一种描述性功能进行的测试
* 每个叶节点为查询指定预测的分类

## Which tree to use?

* 对于数据集中的所有示例，这两个树都将返回相同的预测。

## Occam’s Razor

* 应用与Guess-Who游戏中相同的方法：更喜欢使用较少测试的决策树（浅树）。
* “在相互竞争的假设中，应该选择假设最少的那个”。

## How do we create shallow trees?

* 在根部测试“可疑单词”的树很浅，因为“可疑单词”功能可以将数据完美地分为“垃圾邮件”和“火腿”纯组。
* 相对于目标特征将数据集分为纯集的描述性特征可提供有关目标特征的信息
* 因此，我们可以通过在树的早期测试信息特征来制作浅树。
* 我们所需要做的就是计算集合纯度的度量：熵

## Entropy

* 克劳德·香农（Claude Shannon）的熵模型定义了一组元素的杂质的计算量度。
* 理解集合熵的一种简单方法是，如果要从集合中进行随机选择，则应考虑与猜测结果相关的不确定性。

## Probability and entropy

* 熵与结果的概率有关。
  * 某结果高概率→低熵
  * 某结果低概率→高熵
* 如果我们取概率的对数并将其乘以-1，我们将获得此映射！

## 熵的数学定义

* 香农的熵模型是当我们从集合中进行随机选择时，每个可能结果的概率的对数的加权和。

  ![3]()

## Entropy in poker cards

* 如果仅根据卡片的搭配{♥，♣，♦，♠}来区分52张纸牌，那么熵是多少？

  ​	![4]()

##  Entropy of a message

消息的熵与从中选择消息的集合之间的关系。

![5]()

## Partitions in the spam dataset

![6]()

当我们使用垃圾邮件数据集表中的每个不同描述性功能进行分区时，垃圾邮件数据集中的实例如何拆分。

## Pure subsets

* 我们的直觉是，理想的区分功能会将数据划分为纯子集，其中每个子集中的所有实例具有相同的分类。
  * SUSPICIOUS WORDS完美分割。
  *  UNKNOWN SENDER 混用，但提供了一些信息（“true”大多数情况下为“垃圾邮件”）。
  * CONTAINS IMAGES 没有信息
* 实现此想法的一种方法是使用一种称为信息增益(inforamtion gain)的度量。

## Information Gain

* 描述性特征的信息增益可以理解为通过在该特征上进行测试来预测任务总体熵降低的度量。
* 计算相对于目标特征的原始数据集的熵。 这使我们可以衡量将数据集组织成纯集所需的信息量。
* 对于每个描述性特征，通过使用其特征值对数据集中的实例进行分区来创建结果集，然后对这些集合中每个集合的熵值求和。 这提供了在我们使用描述性功能将实例拆分为纯集后仍需要将信息组织成纯集所需的信息的度量。
* 从原始熵值（在步骤1中计算）中减去剩余的熵值（在步骤2中计算）以得到信息增益

# 计算信息增益

计算信息增益涉及以下三个方程式：

​	![7]()

## 计算目标特征的熵第一步

![8]()

![9]()

## 步骤2：SUSPICIOUS W.特征的分区

* 计算数据集中可疑单词特征的余数。

  ![10]()

  ![11]()

##  步骤2：Unknown sender特征的分区

* 计算数据集中UNKNOWN SENDER功能的余数

  ![12]()

  ![13]()

## 步骤3： 计算信息增益

* 计算数据集中三个描述性特征的信息增益。

![14]()

![15]()

* 这些计算的结果符合我们的直觉。

# The ID3 Algorithm

* ID3算法（迭代二分法3 (Iterative Dichotomizer 3)
* 尝试创建与给出的数据一致的最浅的树。
* ID3算法以递归，深度优先的方式构建树，从根节点开始一直到叶节点。

## How the different nodes are treated 如何对待不同的节点？

* 该算法首先选择最佳的描述性特征来使用信息增益进行测试（即首先要问的最佳问题）。
* 然后将根节点添加到树中，并用选定的测试功能进行标记。
* 然后将训练数据集进行分区，一部分用于测试。
* 对于每个分区，从节点开始都会增加一个分支。
* 然后，使用培训集的相关分区代替完整培训集，并从进一步的测试中排除所选的测试功能，对每个分支重复此过程

## 将连续特征转化为布尔特征

* 处理连续值的描述性特征的最简单方法是通过定义阈值将它们转换为布尔型特征。
* 阈值用于根据实例的连续描述功能的值对实例进行分区。
* 我们如何设置阈值？

## 把实例排序

* 数据集中的实例根据连续要素值进行排序。
* 然后选择排序中具有不同分类的相邻实例作为可能的阈值点
* 通过为这些分类过渡边界中的每一个计算信息增益并选择具有最高信息增益的边界作为阈值，可以找到最佳阈值。

## 将特征视为分类特征

* 设置阈值后，动态创建的新布尔特征可以与其他分类特征竞争以选择作为该节点处的拆分特征。
* 随着树的生长，可以在每个节点上重复此过程

## Vegetation classification dataset 植被分类数据集

* 具有连续高程特征的区域中的植被预测数据集

* 数据集，用于预测按连续ELEVATION功能排序的区域中的植被。

  ![16]()

## 阈值和分区

* 候选ELEVATION阈值的分区集（Part。），熵，余数（Rem。）和信息增益（Info。Gain）：≥750，≥1350，≥2250和≥4175。

  ![17]()

  ![18]()

* 使用ELEVATION≥4175分割数据集后的植被分类决策树。

# 第二次分割

![19]()

使用信息增益为植被分类数据集生成的决策树。

## Parameters to adjust

* maxDepth：一棵树的最大深度。
* maxBins：用于离散化连续特征的最大仓数。 对于任何分类功能，必须为> = 2并且> =类别数。
* impurity: 用于信息增益计算的标准（不区分大小写）。 支持的选项：熵或基尼。